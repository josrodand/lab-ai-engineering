{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construyendo un Sistema de IA Multi-Agente Usando LangGraph y LangSmith\n",
    "\n",
    "¡Bienvenido a esta guía completa sobre cómo construir **flujos de trabajo multi-agente** usando LangGraph! En este notebook, recorreremos el camino desde un agente ReAct básico hasta un sofisticado sistema de soporte al cliente multi-agente. Exploraremos los conceptos clave de LangGraph, aprovecharemos sus librerías pre-construidas e integraremos características avanzadas como la interacción humana en el bucle y la memoria a largo plazo.\n",
    "\n",
    "## ¿Qué es LangGraph?\n",
    "\n",
    "LangGraph es una librería diseñada para construir aplicaciones con estado y múltiples actores utilizando LLMs, cadenas y herramientas. Extiende LangChain permitiéndote definir secuencias de llamadas (cadenas) de manera más robusta, con ciclos, lógica condicional y la capacidad de gestionar transiciones de estado complejas. Esto lo hace ideal para crear flujos de trabajo agentivos donde se requieren múltiples pasos de \"pensar\" y \"actuar\", o donde diferentes agentes especializados necesitan colaborar.\n",
    "\n",
    "## ¿Por qué Multi-Agente?\n",
    "\n",
    "Las arquitecturas multi-agente son poderosas por varias razones:\n",
    "\n",
    "*   **Especialización y Modularidad**: En lugar de un solo agente monolítico intentando manejar todo, un sistema multi-agente se compone de agentes más pequeños y especializados. Cada agente está optimizado para una tarea o dominio específico (por ejemplo, uno para consultas musicales, otro para detalles de facturación). Esto mejora la precisión y el rendimiento dentro de su área de especialización.\n",
    "*   **Flexibilidad y Escalabilidad**: Se pueden añadir nuevas capacidades simplemente integrando un nuevo agente especializado, sin necesidad de reentrenar o modificar significativamente los agentes existentes. Los agentes pueden añadirse, eliminarse o modificarse rápidamente, haciendo el sistema altamente adaptable.\n",
    "*   **Robustez**: Si un agente falla o tiene un bajo rendimiento en su tarea específica, no necesariamente afecta a todo el sistema, ya que los demás agentes pueden seguir funcionando. Esto contribuye a una aplicación más resiliente.\n",
    "*   **Resolución de Problemas Complejos**: Muchos problemas del mundo real requieren diferentes tipos de experiencia. Un sistema multi-agente puede imitar la colaboración de un equipo humano, dividiendo consultas complejas en sub-problemas que son manejados por el experto más adecuado.\n",
    "\n",
    "## Nuestro Escenario de Soporte al Cliente\n",
    "\n",
    "Simularemos un ejemplo realista de soporte al cliente para una tienda de música digital. El agente interactuará con la [base de datos Chinook](https://www.sqlitetutorial.net/sqlite-sample-database/), que contiene información completa sobre clientes, facturas y un catálogo musical.\n",
    "\n",
    "Nuestra arquitectura final se verá algo así:\n",
    "\n",
    "![Diagrama de Arquitectura](images/architecture.png) \n",
    "\n",
    "Como puedes ver, el sistema comienza con un paso de **verificación del cliente** (con intervención humana), luego **carga las preferencias del usuario** desde la memoria a largo plazo. Un **agente supervisor** dirige inteligentemente la consulta al sub-agente especializado apropiado: ya sea el **sub-agente de catálogo musical** o el **sub-agente de información de facturas**. Finalmente, el sistema **guarda cualquier nueva preferencia del usuario** en la memoria a largo plazo antes de proporcionar una respuesta.\n",
    "\n",
    "Para profundizar en los conceptos básicos de LangGraph y aprender sobre nuestro framework, visita nuestra [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trabajo: Configuración\n",
    "\n",
    "Antes de sumergirnos en la construcción de nuestro sistema multi-agente, configuremos nuestro entorno. Esto implica cargar las variables de entorno necesarias, conectar con nuestra base de datos de ejemplo e inicializar nuestros almacenes de memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargando Variables de Entorno\n",
    "\n",
    "Para comenzar, cargaremos nuestras claves de API y otra configuración desde un archivo `.env`. Esto mantiene la información sensible fuera del código. En este ejemplo usaremos los modelos de OpenAI, pero LangGraph es independiente del modelo, por lo que puedes reemplazar fácilmente `ChatOpenAI` por otros proveedores de `ChatModel`, como Azure OpenAI, Anthropic o Google Gemini.\n",
    "\n",
    "Asegúrate de que tu archivo `.env` incluya todas las claves especificadas en el archivo `.env.example` (por ejemplo, `OPENAI_API_KEY`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "from langchain_openai import AzureChatOpenAI # Import the OpenAI chat model\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"UOC_API_KEY\")\n",
    "api_base = os.getenv(\"UOC_ENDPOINT\")\n",
    "deployment = os.getenv(\"UOC_MODEL_NAME\")\n",
    "api_version = os.getenv(\"UOC_API_VERSION\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    azure_endpoint=api_base,\n",
    "    deployment_name=deployment,\n",
    "    api_version=api_version,\n",
    "    # temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargando Datos de Cliente de Muestra (Base de Datos Chinook)\n",
    "\n",
    "Nuestro agente de soporte al cliente interactuará con una base de datos para recuperar información. Usaremos la [base de datos Chinook](https://www.sqlitetutorial.net/sqlite-sample-database/), una base de datos de ejemplo muy utilizada que contiene tablas relacionadas con información de clientes, historial de compras y un catálogo musical (artistas, álbumes, canciones, géneros). Esto proporciona un conjunto de datos completo para que nuestro agente pueda hacer consultas.\n",
    "\n",
    "Cargaremos esta base de datos SQLite en memoria para un acceso rápido durante la ejecución del notebook. Esto evita el almacenamiento persistente en archivos y simplifica la configuración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 # Standard Python library for SQLite database interaction\n",
    "import requests # Library for making HTTP requests (to download the SQL script)\n",
    "from langchain_community.utilities.sql_database import SQLDatabase # LangChain utility to interact with SQL databases\n",
    "from sqlalchemy import create_engine # SQLAlchemy function to create a database engine\n",
    "from sqlalchemy.pool import StaticPool # SQLAlchemy connection pool class for in-memory databases\n",
    "\n",
    "def get_engine_for_chinook_db():\n",
    "    \"\"\"Pull sql file, populate in-memory database, and create engine.\"\"\"\n",
    "    # URL to the raw SQL script for the Chinook database\n",
    "    url = \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\"\n",
    "    \n",
    "    # Fetch the SQL script content from the URL\n",
    "    response = requests.get(url)\n",
    "    sql_script = response.text\n",
    "\n",
    "    # Create an in-memory SQLite database connection.\n",
    "    # `check_same_thread=False` is important for SQLAlchemy's StaticPool.\n",
    "    connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
    "    \n",
    "    # Execute the SQL script to populate the in-memory database with Chinook data\n",
    "    connection.executescript(sql_script)\n",
    "    \n",
    "    # Create a SQLAlchemy engine for the in-memory SQLite database.\n",
    "    # `creator=lambda: connection` tells SQLAlchemy how to get a new connection.\n",
    "    # `poolclass=StaticPool` is used for in-memory databases, ensuring the same connection is reused.\n",
    "    # `connect_args` are passed directly to the `sqlite3.connect` function.\n",
    "    return create_engine(\n",
    "        \"sqlite://\",\n",
    "        creator=lambda: connection,\n",
    "        poolclass=StaticPool,\n",
    "        connect_args={\"check_same_thread\": False},\n",
    "    )\n",
    "\n",
    "# Get the SQLAlchemy engine for our Chinook database\n",
    "engine = get_engine_for_chinook_db()\n",
    "\n",
    "# Create a LangChain SQLDatabase utility instance from the engine.\n",
    "# This utility will help our agents interact with the database via SQL queries.\n",
    "db = SQLDatabase(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración de Memoria a Corto y Largo Plazo\n",
    "\n",
    "La memoria es un componente crucial para construir agentes inteligentes. LangGraph proporciona mecanismos para gestionar tanto la memoria a corto plazo como la memoria a largo plazo en nuestros flujos de trabajo.\n",
    "\n",
    "* **Memoria a Corto Plazo (Checkpointer)**: Preserva el estado de un hilo conversacional específico. Permite que el agente mantenga el contexto y continúe desde donde lo dejó dentro de *una sola conversación*. Si la consulta de un usuario requiere varios turnos o intervención humana, el checkpointer garantiza que el estado del grafo se guarde y pueda reanudarse.\n",
    "\n",
    "  * Usamos `MemorySaver` para una memoria a corto plazo en memoria, adecuada para fines de demostración. En producción, normalmente se usaría un checkpointer persistente (por ejemplo, SQL, Redis).\n",
    "\n",
    "* **Memoria a Largo Plazo (InMemoryStore)**: Permite almacenar y recuperar información *entre conversaciones* o a lo largo de diferentes sesiones del mismo usuario. En nuestro escenario, la usaremos para guardar preferencias del usuario, permitiendo la personalización. Por ejemplo, si un usuario menciona su género musical favorito, esta información puede almacenarse y utilizarse en futuras interacciones.\n",
    "\n",
    "  * Usamos `InMemoryStore` para almacenamiento en memoria a largo plazo. Al igual que con los checkpointers, en un entorno de producción se utilizaría un almacenamiento persistente (por ejemplo, una base de datos vectorial o un almacén clave-valor).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver # For short-term memory (thread-level state persistence)\n",
    "from langgraph.store.memory import InMemoryStore # For long-term memory (storing user preferences)\n",
    "\n",
    "# Initializing `InMemoryStore` for long-term memory. \n",
    "# This store will hold user-specific data like music preferences across sessions.\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Initializing `MemorySaver` for short-term (thread-level) memory. \n",
    "# This checkpointer saves the graph's state after each step, allowing for restarts or interruptions within a thread.\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Construyendo Subagentes ReAct\n",
    "\n",
    "Nuestro sistema multiagente estará compuesto por subagentes especializados. Comenzaremos construyendo dos agentes ReAct fundamentales: uno desde cero para entender sus componentes principales, y otro utilizando las utilidades preconstruidas de LangGraph para un desarrollo más ágil.\n",
    "\n",
    "### 1.1 Construyendo un Agente ReAct Desde Cero: El Subagente del Catálogo Musical\n",
    "\n",
    "Nuestro primer subagente se encargará de atender consultas de clientes relacionadas con el catálogo de la tienda de música. Este agente utilizará un conjunto de herramientas para obtener información sobre artistas, álbumes, canciones y géneros desde la base de datos Chinook.\n",
    "\n",
    "El marco ReAct (Razonamiento y Acción) es un patrón popular para construir agentes que alternan pasos de razonamiento con pasos de acción (uso de herramientas). El agente **razona** sobre qué herramienta usar, **actúa** llamando a esa herramienta, y luego **observa** el resultado para **razonar** nuevamente.\n",
    "\n",
    "![Arquitectura del Subagente Musical](images/music_subagent.png)\n",
    "\n",
    "#### Estado\n",
    "\n",
    "En LangGraph, el **Estado** es un concepto fundamental. Actúa como la memoria compartida del agente: una estructura de datos que se pasa entre los nodos del grafo. Cada nodo recibe el estado actual, ejecuta su lógica y devuelve actualizaciones al estado, que luego se convierten en la entrada para el siguiente nodo. Este flujo continuo de información a través del estado permite que el grafo mantenga el contexto y acumule información a medida que avanza.\n",
    "\n",
    "Para nuestro agente de soporte al cliente, el estado rastreará los siguientes elementos clave:\n",
    "\n",
    "1. `customer_id`: Una cadena que representa el ID del cliente que interactúa con el agente. Esto es crucial para consultas personalizadas (por ejemplo, revisar el historial de facturas).\n",
    "2. `messages`: Una lista anotada de objetos `AnyMessage`. Esto forma el historial de la conversación, incluyendo entradas del usuario, respuestas del agente y resultados de herramientas. `add_messages` se encarga de agregar nuevos mensajes a la lista, manteniendo el flujo conversacional.\n",
    "3. `loaded_memory`: Una cadena que almacenará cualquier preferencia del usuario o información relevante cargada desde la memoria a largo plazo. Esto permite que el agente personalice las respuestas según interacciones pasadas.\n",
    "4. `remaining_steps`: Un objeto `RemainingSteps`. Esto forma parte del estado gestionado por LangGraph y ayuda a rastrear el número de pasos restantes antes de alcanzar el límite de recursión, previniendo bucles infinitos en grafos cíclicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict # For defining dictionaries with type hints\n",
    "from typing import Annotated, List # For type hinting lists and adding annotations\n",
    "from langgraph.graph.message import AnyMessage, add_messages # For managing messages in the graph state\n",
    "from langgraph.managed.is_last_step import RemainingSteps # For tracking recursion limits\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"Represents the state of our LangGraph agent.\"\"\"\n",
    "    # customer_id: Stores the unique identifier for the current customer.\n",
    "    customer_id: str\n",
    "    \n",
    "    # messages: A list of messages that form the conversation history.\n",
    "    # Annotated with `add_messages` to ensure new messages are appended rather than overwritten.\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    \n",
    "    # loaded_memory: Stores information loaded from the long-term memory store, \n",
    "    # typically user preferences or historical context.\n",
    "    loaded_memory: str\n",
    "    \n",
    "    # remaining_steps: Used by LangGraph to track the number of allowed steps \n",
    "    # to prevent infinite loops in cyclic graphs.\n",
    "    remaining_steps: RemainingSteps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Herramientas\n",
    "\n",
    "Las herramientas son funcionalidades externas que un modelo de lenguaje (LLM) puede invocar para ampliar sus capacidades más allá de la simple generación de texto. Estas pueden ser APIs, consultas a bases de datos o cualquier función arbitraria en Python. En nuestro subagente del catálogo musical, definiremos un conjunto de herramientas que interactúan con la base de datos Chinook para obtener información relacionada con la música.\n",
    "\n",
    "Utilizamos el decorador `@tool` de LangChain para exponer fácilmente funciones de Python como herramientas que nuestro LLM puede aprender a usar. Este decorador genera automáticamente un esquema que el modelo puede comprender, lo que le permite decidir cuándo y cómo llamar a la herramienta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool # Decorator to define a function as a LangChain tool\n",
    "import ast # Module to safely evaluate strings containing Python literal structures\n",
    "\n",
    "@tool\n",
    "def get_albums_by_artist(artist: str):\n",
    "    \"\"\"Get albums by an artist.\"\"\"\n",
    "    # Execute a SQL query to retrieve album titles and artist names\n",
    "    # from the Album and Artist tables, joining them and filtering by artist name.\n",
    "    # `db.run` is a utility from LangChain's SQLDatabase to execute queries.\n",
    "    # `include_columns=True` ensures column names are included in the result for better readability.\n",
    "    return db.run(\n",
    "        f\"\"\"\n",
    "        SELECT Album.Title, Artist.Name \n",
    "        FROM Album \n",
    "        JOIN Artist ON Album.ArtistId = Artist.ArtistId \n",
    "        WHERE Artist.Name LIKE '%{artist}%';\n",
    "        \"\"\",\n",
    "        include_columns=True\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def get_tracks_by_artist(artist: str):\n",
    "    \"\"\"Get songs by an artist (or similar artists).\"\"\"\n",
    "    # Execute a SQL query to find tracks (songs) by a given artist, or similar artists.\n",
    "    # It joins Album, Artist, and Track tables to get song names and artist names.\n",
    "    return db.run(\n",
    "        f\"\"\"\n",
    "        SELECT Track.Name as SongName, Artist.Name as ArtistName \n",
    "        FROM Album \n",
    "        LEFT JOIN Artist ON Album.ArtistId = Artist.ArtistId \n",
    "        LEFT JOIN Track ON Track.AlbumId = Album.AlbumId \n",
    "        WHERE Artist.Name LIKE '%{artist}%';\n",
    "        \"\"\",\n",
    "        include_columns=True\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def get_songs_by_genre(genre: str):\n",
    "    \"\"\"\n",
    "    Fetch songs from the database that match a specific genre.\n",
    "    \n",
    "    Args:\n",
    "        genre (str): The genre of the songs to fetch.\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: A list of songs that match the specified genre.\n",
    "    \"\"\"\n",
    "    # First, find the GenreId for the given genre name.\n",
    "    genre_id_query = f\"SELECT GenreId FROM Genre WHERE Name LIKE '%{genre}%'\"\n",
    "    genre_ids = db.run(genre_id_query)\n",
    "    \n",
    "    # If no genre IDs are found, return an informative message.\n",
    "    if not genre_ids:\n",
    "        return f\"No songs found for the genre: {genre}\"\n",
    "    \n",
    "    # Safely evaluate the string result from db.run to get a list of tuples.\n",
    "    genre_ids = ast.literal_eval(genre_ids)\n",
    "    # Extract just the GenreId values and join them into a comma-separated string for the IN clause.\n",
    "    genre_id_list = \", \".join(str(gid[0]) for gid in genre_ids)\n",
    "\n",
    "    # Construct the query to get songs for the found genre IDs.\n",
    "    # It joins Track, Album, and Artist tables and limits the results to 8.\n",
    "    songs_query = f\"\"\"\n",
    "        SELECT Track.Name as SongName, Artist.Name as ArtistName\n",
    "        FROM Track\n",
    "        LEFT JOIN Album ON Track.AlbumId = Album.AlbumId\n",
    "        LEFT JOIN Artist ON Album.ArtistId = Artist.ArtistId\n",
    "        WHERE Track.GenreId IN ({genre_id_list})\n",
    "        GROUP BY Artist.Name\n",
    "        LIMIT 8;\n",
    "    \"\"\"\n",
    "    songs = db.run(songs_query, include_columns=True)\n",
    "    \n",
    "    # If no songs are found for the genre, return an informative message.\n",
    "    if not songs:\n",
    "        return f\"No songs found for the genre: {genre}\"\n",
    "        \n",
    "    # Safely evaluate the string result and format it into a list of dictionaries.\n",
    "    formatted_songs = ast.literal_eval(songs)\n",
    "    return [\n",
    "        {\"Song\": song[\"SongName\"], \"Artist\": song[\"ArtistName\"]}\n",
    "        for song in formatted_songs\n",
    "    ]\n",
    "\n",
    "@tool\n",
    "def check_for_songs(song_title):\n",
    "    \"\"\"Check if a song exists by its name.\"\"\"\n",
    "    # Execute a SQL query to check for the existence of a song by its title.\n",
    "    return db.run(\n",
    "        f\"\"\"\n",
    "        SELECT * FROM Track WHERE Name LIKE '%{song_title}%';\n",
    "        \"\"\",\n",
    "        include_columns=True\n",
    "    )\n",
    "\n",
    "# Aggregate all music-related tools into a list.\n",
    "music_tools = [get_albums_by_artist, get_tracks_by_artist, get_songs_by_genre, check_for_songs]\n",
    "\n",
    "# Bind the tools to our ChatOpenAI model.\n",
    "# This step configures the LLM so it knows about the available tools and their schemas,\n",
    "# allowing it to generate tool calls when appropriate based on the user's query.\n",
    "llm_with_music_tools = llm.bind_tools(music_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nodos\n",
    "\n",
    "En LangGraph, los **Nodos** son los bloques fundamentales de tu grafo. Son, esencialmente, funciones en Python (o JS/TS) que toman el `State` (estado) del grafo como entrada, ejecutan cierta lógica (por ejemplo, invocar un LLM, llamar a una herramienta, actualizar datos) y devuelven actualizaciones al `State`.\n",
    "\n",
    "Para nuestro agente ReAct, definiremos dos tipos principales de nodos:\n",
    "\n",
    "1. **`music_assistant` (Nodo de Razonamiento)**: Este nodo es un modelo de lenguaje (LLM) responsable del **razonamiento**. Toma el historial actual de la conversación y la consulta del usuario, considera las herramientas disponibles y decide la próxima mejor acción. Esta acción puede ser invocar una herramienta o, si la consulta ya está resuelta, generar una respuesta final.\n",
    "\n",
    "2. **`music_tool_node` (Nodo de Acción)**: Este nodo se encarga de la **acción**. Cuando el `music_assistant` decide utilizar una herramienta, el `music_tool_node` recibe la llamada a la herramienta, ejecuta la función correspondiente y luego devuelve la salida de esa herramienta al estado del grafo. LangGraph proporciona una utilidad llamada `ToolNode` que facilita la ejecución automática de herramientas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode # Pre-built node for executing tools\n",
    "\n",
    "# Create a ToolNode instance. This node will automatically execute any tool calls \n",
    "# generated by an LLM that is bound to these tools.\n",
    "music_tool_node = ToolNode(music_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage, SystemMessage, HumanMessage # Message types for conversation history\n",
    "from langchain_core.runnables import RunnableConfig # For configuration parameters passed to runnables\n",
    "\n",
    "# Define the system prompt for the music assistant.\n",
    "# This prompt provides instructions and persona for the LLM.\n",
    "# It emphasizes the agent's role, core responsibilities, and search guidelines.\n",
    "# The `memory` placeholder allows us to inject user preferences from long-term memory.\n",
    "def generate_music_assistant_prompt(memory: str = \"None\") -> str:\n",
    "    return f\"\"\"\n",
    "    You are a member of the assistant team, your role specifically is to focused on helping customers discover and learn about music in our digital catalog. \n",
    "    If you are unable to find playlists, songs, or albums associated with an artist, it is okay. \n",
    "    Just inform the customer that the catalog does not have any playlists, songs, or albums associated with that artist.\n",
    "    You also have context on any saved user preferences, helping you to tailor your response. \n",
    "    \n",
    "    CORE RESPONSIBILITIES:\n",
    "    - Search and provide accurate information about songs, albums, artists, and playlists\n",
    "    - Offer relevant recommendations based on customer interests\n",
    "    - Handle music-related queries with attention to detail\n",
    "    - Help customers discover new music they might enjoy\n",
    "    - You are routed only when there are questions related to music catalog; ignore other questions. \n",
    "    \n",
    "    SEARCH GUIDELINES:\n",
    "    1. Always perform thorough searches before concluding something is unavailable\n",
    "    2. If exact matches aren't found, try:\n",
    "       - Checking for alternative spellings\n",
    "       - Looking for similar artist names\n",
    "       - Searching by partial matches\n",
    "       - Checking different versions/remixes\n",
    "    3. When providing song lists:\n",
    "       - Include the artist name with each song\n",
    "       - Mention the album when relevant\n",
    "       - Note if it's part of any playlists\n",
    "       - Indicate if there are multiple versions\n",
    "    \n",
    "    Additional context is provided below: \n",
    "\n",
    "    Prior saved user preferences: {memory}\n",
    "    \n",
    "    Message history is also attached.  \n",
    "    \"\"\"\n",
    "\n",
    "# Define the music_assistant node function.\n",
    "# This function receives the current `State` and `RunnableConfig`.\n",
    "def music_assistant(state: State, config: RunnableConfig): \n",
    "\n",
    "    # Fetch long-term memory (user preferences) from the state.\n",
    "    # If `loaded_memory` is not present in the state, default to \"None\".\n",
    "    memory = \"None\" \n",
    "    if \"loaded_memory\" in state: \n",
    "        memory = state[\"loaded_memory\"]\n",
    "\n",
    "    # Generate the system prompt for the music assistant, injecting the loaded memory.\n",
    "    music_assistant_prompt = generate_music_assistant_prompt(memory)\n",
    "\n",
    "    # Invoke the LLM (`llm_with_music_tools`) with the system prompt and the current message history.\n",
    "    # The LLM will decide whether to call a tool or generate a final response.\n",
    "    response = llm_with_music_tools.invoke([SystemMessage(music_assistant_prompt)] + state[\"messages\"])\n",
    "    \n",
    "    # Update the state by appending the LLM's response to the `messages` list.\n",
    "    # The `add_messages` annotation in `State` ensures this is appended correctly.\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aristas (Edges)\n",
    "\n",
    "Las **aristas** son las conexiones entre nodos en un grafo de LangGraph. Definen el flujo y la secuencia de ejecución dentro de tu aplicación.\n",
    "\n",
    "* **Aristas Normales**: Son deterministas, lo que significa que siempre conducen de un nodo específico a otro también específico. Por ejemplo, `graph.add_edge(\"node_A\", \"node_B\")` indica que, una vez que `node_A` finaliza, `node_B` se ejecutará siempre a continuación.\n",
    "\n",
    "* **Aristas Condicionales**: Permiten un enrutamiento dinámico. En lugar de tener un destino fijo, una arista condicional utiliza una función (llamada \"router\" o \"función condicional\") que inspecciona el `State` actual y devuelve una cadena correspondiente al nombre del próximo nodo a visitar. Esto permite tomar decisiones flexibles e inteligentes sobre la ruta del flujo de trabajo.\n",
    "\n",
    "Para nuestro agente ReAct, necesitaremos una **arista condicional** después del nodo `music_assistant`. Esta arista determinará:\n",
    "\n",
    "* Si `music_assistant` decidió invocar una herramienta, deberíamos dirigirnos al nodo `music_tool_node` para ejecutarla.\n",
    "* Si `music_assistant` generó una respuesta final comprensible para el usuario (es decir, sin llamadas a herramientas), deberíamos ir al nodo `END`, finalizando la ejecución del subagente porque la consulta ya ha sido resuelta.\n",
    "\n",
    "La función `should_continue` implementa esta lógica condicional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a conditional edge function named `should_continue`.\n",
    "# This function determines the next step in the graph based on the LLM's response.\n",
    "def should_continue(state: State, config: RunnableConfig):\n",
    "    # Get the list of messages from the current state.\n",
    "    messages = state[\"messages\"]\n",
    "    # Get the last message, which is the response from the `music_assistant` LLM.\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if the last message contains any tool calls.\n",
    "    # LLMs generate `tool_calls` when they decide to use a function.\n",
    "    if not last_message.tool_calls:\n",
    "        # If there are no tool calls, it means the LLM has generated a final answer.\n",
    "        # In this case, the sub-agent's work is done, so we return \"end\" to signal completion.\n",
    "        return \"end\"\n",
    "    # Otherwise, if there are tool calls,\n",
    "    else:\n",
    "        # We need to execute the tool(s). So, we return \"continue\" to route to the tool execution node.\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¡Compilar el Grafo!\n",
    "\n",
    "Ahora que ya hemos definido nuestro `State` (qué datos fluyen), los `Nodes` (qué acciones se ejecutan) y las `Edges` (cómo fluye el control), podemos ensamblar todo en un flujo de trabajo completo con LangGraph. Este proceso se denomina **compilación**.\n",
    "\n",
    "La clase `StateGraph` se utiliza para definir la estructura de nuestro agente. Agregamos nodos y aristas, y luego *compilamos* el grafo. La compilación convierte el grafo definido en un objeto ejecutable, listo para ser invocado.\n",
    "\n",
    "Métodos clave utilizados:\n",
    "\n",
    "* `StateGraph(State)`: Inicializa un grafo con nuestro esquema de estado definido.\n",
    "* `add_node(nombre, función_nodo)`: Agrega un nodo al grafo, asociando un nombre con una función de Python invocable.\n",
    "* `add_edge(origen, destino)`: Crea una arista directa e incondicional desde el nodo `origen` al nodo `destino`.\n",
    "* `add_conditional_edges(origen, función_condicional, mapeo)`: Crea una arista dinámica. La `función_condicional` se llama para determinar el siguiente nodo según su valor de retorno, el cual debe coincidir con una clave en el diccionario `mapeo`.\n",
    "\n",
    "  * `START`: Punto de entrada especial que marca el inicio de la ejecución del grafo.\n",
    "  * `END`: Punto de salida especial que indica el final de la ejecución del grafo.\n",
    "* `compile(name, checkpointer, store)`: Finaliza el grafo.\n",
    "\n",
    "  * `name`: Identificador único para el grafo compilado.\n",
    "  * `checkpointer`: Mecanismo de memoria a corto plazo (`MemorySaver`) para guardar y reanudar el estado del grafo.\n",
    "  * `store`: Mecanismo de memoria a largo plazo (`InMemoryStore`) para mantener datos entre sesiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END # Core LangGraph classes and special node names\n",
    "from utils import show_graph # Utility function to visualize the graph (assumed to be in a utils.py file)\n",
    "\n",
    "# Initialize a StateGraph with our defined `State` schema.\n",
    "# This tells LangGraph how the data will flow and be managed within the graph.\n",
    "music_workflow = StateGraph(State)\n",
    "\n",
    "# Add the 'music_assistant' node to the graph.\n",
    "# This node is responsible for the LLM's reasoning and generating tool calls or final responses.\n",
    "music_workflow.add_node(\"music_assistant\", music_assistant)\n",
    "\n",
    "# Add the 'music_tool_node' to the graph.\n",
    "# This node is responsible for executing the tools when requested by the LLM.\n",
    "music_workflow.add_node(\"music_tool_node\", music_tool_node)\n",
    "\n",
    "\n",
    "# Define the starting point of the graph.\n",
    "# All queries will initially enter the 'music_assistant' node.\n",
    "music_workflow.add_edge(START, \"music_assistant\")\n",
    "\n",
    "# Add a conditional edge from 'music_assistant'.\n",
    "# The `should_continue` function will be called to determine the next node.\n",
    "music_workflow.add_conditional_edges(\n",
    "    \"music_assistant\", # Source node\n",
    "    should_continue,   # Conditional function to call\n",
    "    {\n",
    "        # If `should_continue` returns \"continue\", route to `music_tool_node`.\n",
    "        \"continue\": \"music_tool_node\",\n",
    "        # If `should_continue` returns \"end\", terminate the graph execution.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add a normal edge from 'music_tool_node' back to 'music_assistant'.\n",
    "# After a tool is executed, the result is fed back to the LLM for further reasoning \n",
    "# or to formulate a final response (ReAct loop).\n",
    "music_workflow.add_edge(\"music_tool_node\", \"music_assistant\")\n",
    "\n",
    "# Compile the graph into a runnable object.\n",
    "# `name`: A unique identifier for this compiled graph (useful for debugging and logging).\n",
    "# `checkpointer`: The short-term memory mechanism (MemorySaver) for thread-specific state.\n",
    "# `store`: The long-term memory mechanism (InMemoryStore) for persistent user data.\n",
    "music_catalog_subagent = music_workflow.compile(name=\"music_catalog_subagent\", checkpointer=checkpointer, store = in_memory_store)\n",
    "\n",
    "# Display a visualization of the compiled graph.\n",
    "show_graph(music_catalog_subagent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba del Subagente del Catálogo Musical\n",
    "\n",
    "Ahora que hemos compilado nuestro primer subagente ReAct, vamos a probar su funcionamiento. Simularemos una consulta de un cliente y observaremos cómo el agente la procesa, posiblemente invocando herramientas y proporcionando una respuesta.\n",
    "\n",
    "Elementos clave en la prueba:\n",
    "\n",
    "* `uuid.uuid4()`: Genera un ID de conversación único para cada sesión. Esto es crucial para que el *checkpointer* mantenga estados separados para conversaciones distintas.\n",
    "\n",
    "  * En una aplicación real, este `thread_id` normalmente correspondería a una sesión de usuario o a un identificador único de conversación.\n",
    "\n",
    "* `config`: Un diccionario que se pasa al método `invoke`, que contiene parámetros configurables como el `thread_id`. El *checkpointer* usa este ID para cargar y guardar el estado correcto del grafo.\n",
    "\n",
    "* `invoke()`: Inicia la ejecución del grafo LangGraph. Recibe como entrada inicial los `messages` definidos en el `State` y la configuración (`config`).\n",
    "\n",
    "* `pretty_print()`: Un método utilitario (suponiendo que esté definido para `AnyMessage` u objeto similar) que muestra los mensajes en un formato legible, indicando claramente los roles (usuario, agente, herramienta) y el contenido.\n",
    "\n",
    "Esta prueba nos permitirá verificar si el subagente puede manejar consultas reales sobre el catálogo musical, utilizar herramientas cuando sea necesario y generar respuestas útiles y coherentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid # Module for generating unique identifiers\n",
    "\n",
    "# Generate a unique thread ID for this conversation.\n",
    "# This ensures that the conversation state is isolated and can be resumed later.\n",
    "thread_id = uuid.uuid4()\n",
    "\n",
    "# Define the customer's question.\n",
    "question = \"I like the Rolling Stones. What songs do you recommend by them or by other artists that I might like?\"\n",
    "\n",
    "# Create the configuration dictionary for invoking the graph.\n",
    "# The `thread_id` is essential for the checkpointer to manage state.\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Invoke the `music_catalog_subagent` with the initial human message and configuration.\n",
    "# The `invoke` method runs the graph to completion and returns the final state.\n",
    "result = music_catalog_subagent.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "\n",
    "# Iterate through the messages in the final state and print them for observation.\n",
    "# `pretty_print()` provides a formatted output of the message content and role.\n",
    "for message in result[\"messages\"]:\n",
    "   message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Construcción de un Agente ReAct Usando Funcionalidades Preconstruidas de LangGraph: El Subagente de Información de Facturas\n",
    "\n",
    "Aunque construir agentes desde cero proporciona un entendimiento profundo del funcionamiento interno, LangGraph también ofrece **librerías preconstruidas** para arquitecturas comunes. Estas soluciones predefinidas permiten desarrollar rápidamente agentes con patrones estándar como ReAct, eliminando gran parte del código repetitivo (*boilerplate*).\n",
    "\n",
    "Puedes consultar la lista completa de librerías preconstruidas aquí:\n",
    "👉 [LangGraph Pre-built Libraries](https://langchain-ai.github.io/langgraph/prebuilt/#available-libraries)\n",
    "\n",
    "En esta sección, demostraremos cómo crear nuestro segundo subagente, el **Subagente de Información de Facturas**, utilizando la utilidad preconstruida `create_react_agent`. Este agente se encargará de manejar todas las consultas de clientes relacionadas con facturas y compras anteriores.\n",
    "\n",
    "![Arquitectura del Subagente de Facturas](images/invoice_subagent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de Herramientas y Prompt para el Subagente de Facturación\n",
    "\n",
    "Al igual que nuestro subagente musical, el subagente de facturación requiere su propio conjunto de herramientas especializadas y un prompt personalizado. Estas herramientas interactuarán con la base de datos Chinook para recuperar información específica de facturación.\n",
    "\n",
    "Cada herramienta está diseñada para responder a un tipo específico de consulta que un cliente podría tener sobre sus facturas. Los prompts guiarán al LLM para que comprenda su rol y utilice estas herramientas de manera efectiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.12.11)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\Users\\jlran\\dev\\lab-ai-engineering\\.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool # Import the tool decorator again\n",
    "\n",
    "@tool \n",
    "def get_invoices_by_customer_sorted_by_date(customer_id: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Look up all invoices for a customer using their ID.\n",
    "    The invoices are sorted in descending order by invoice date, which helps when the customer wants to view their most recent/oldest invoice, or if \n",
    "    they want to view invoices within a specific date range.\n",
    "    \n",
    "    Args:\n",
    "        customer_id (str): customer_id, which serves as the identifier.\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: A list of invoices for the customer.\n",
    "    \"\"\"\n",
    "    # Executes a SQL query to retrieve all invoice details for a given customer ID,\n",
    "    # ordered by invoice date in descending order (most recent first).\n",
    "    return db.run(f\"SELECT * FROM Invoice WHERE CustomerId = {customer_id} ORDER BY InvoiceDate DESC;\")\n",
    "\n",
    "\n",
    "@tool \n",
    "def get_invoices_sorted_by_unit_price(customer_id: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Use this tool when the customer wants to know the details of one of their invoices based on the unit price/cost of the invoice.\n",
    "    This tool looks up all invoices for a customer, and sorts the unit price from highest to lowest. In order to find the invoice associated with the customer, \n",
    "    we need to know the customer ID.\n",
    "    \n",
    "    Args:\n",
    "        customer_id (str): customer_id, which serves as the identifier.\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: A list of invoices sorted by unit price.\n",
    "    \"\"\"\n",
    "    # Executes a SQL query to retrieve invoice details along with the unit price of items in those invoices,\n",
    "    # for a given customer ID, ordered by unit price in descending order (highest unit price first).\n",
    "    query = f\"\"\"\n",
    "        SELECT Invoice.*, InvoiceLine.UnitPrice\n",
    "        FROM Invoice\n",
    "        JOIN InvoiceLine ON Invoice.InvoiceId = InvoiceLine.InvoiceId\n",
    "        WHERE Invoice.CustomerId = {customer_id}\n",
    "        ORDER BY InvoiceLine.UnitPrice DESC;\n",
    "    \"\"\"\n",
    "    return db.run(query)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_employee_by_invoice_and_customer(invoice_id: str, customer_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    This tool will take in an invoice ID and a customer ID and return the employee information associated with the invoice.\n",
    "\n",
    "    Args:\n",
    "        invoice_id (int): The ID of the specific invoice.\n",
    "        customer_id (str): customer_id, which serves as the identifier.\n",
    "\n",
    "    Returns:\n",
    "        dict: Information about the employee associated with the invoice.\n",
    "    \"\"\"\n",
    "\n",
    "    # Executes a SQL query to find the employee associated with a specific invoice and customer.\n",
    "    # It joins Employee, Customer, and Invoice tables to retrieve employee first name, title, and email.\n",
    "    query = f\"\"\"\n",
    "        SELECT Employee.FirstName, Employee.Title, Employee.Email\n",
    "        FROM Employee\n",
    "        JOIN Customer ON Customer.SupportRepId = Employee.EmployeeId\n",
    "        JOIN Invoice ON Invoice.CustomerId = Customer.CustomerId\n",
    "        WHERE Invoice.InvoiceId = ({invoice_id}) AND Invoice.CustomerId = ({customer_id});\n",
    "    \"\"\"\n",
    "    \n",
    "    employee_info = db.run(query, include_columns=True)\n",
    "    \n",
    "    # Checks if any employee information was found.\n",
    "    if not employee_info:\n",
    "        return f\"No employee found for invoice ID {invoice_id} and customer identifier {customer_id}.\"\n",
    "    return employee_info\n",
    "\n",
    "# Aggregate all invoice-related tools into a list.\n",
    "invoice_tools = [get_invoices_by_customer_sorted_by_date, get_invoices_sorted_by_unit_price, get_employee_by_invoice_and_customer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the invoice information sub-agent.\n",
    "# This prompt sets the persona and core responsibilities for the LLM within this sub-agent's domain.\n",
    "# It explicitly lists the tools available to this agent and provides guidelines for their use.\n",
    "invoice_subagent_prompt = \"\"\"\n",
    "    You are a subagent among a team of assistants. You are specialized for retrieving and processing invoice information. You are routed for invoice-related portion of the questions, so only respond to them.. \n",
    "\n",
    "    You have access to three tools. These tools enable you to retrieve and process invoice information from the database. Here are the tools:\n",
    "    - get_invoices_by_customer_sorted_by_date: This tool retrieves all invoices for a customer, sorted by invoice date.\n",
    "    - get_invoices_sorted_by_unit_price: This tool retrieves all invoices for a customer, sorted by unit price.\n",
    "    - get_employee_by_invoice_and_customer: This tool retrieves the employee information associated with an invoice and a customer.\n",
    "    \n",
    "    If you are unable to retrieve the invoice information, inform the customer you are unable to retrieve the information, and ask if they would like to search for something else.\n",
    "    \n",
    "    CORE RESPONSIBILITIES:\n",
    "    - Retrieve and process invoice information from the database\n",
    "    - Provide detailed information about invoices, including customer details, invoice dates, total amounts, employees associated with the invoice, etc. when the customer asks for it.\n",
    "    - Always maintain a professional, friendly, and patient demeanor\n",
    "    \n",
    "    You may have additional context that you should use to help answer the customer's query. It will be provided to you below:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de la Librería Preconstruida: `create_react_agent`\n",
    "\n",
    "Ahora vamos a ensamblar nuestro subagente de facturación utilizando la función `create_react_agent` de LangGraph. Esta utilidad abstrae la definición de nodos y aristas para un bucle ReAct estándar, lo que nos permite crear rápidamente un agente ejecutable.\n",
    "\n",
    "La función `create_react_agent` se encarga de:\n",
    "\n",
    "* Vincular las `tools` proporcionadas al `model`.\n",
    "* Configurar el LLM (`model`) como nodo de razonamiento.\n",
    "* Configurar un `ToolNode` para ejecutar las herramientas.\n",
    "* Definir la lógica condicional (aristas) para alternar entre el LLM y las herramientas hasta que se produzca una respuesta final o se alcance un límite de recursión.\n",
    "\n",
    "Le proporcionamos:\n",
    "\n",
    "* `model`: El LLM a utilizar (nuestra instancia de `ChatOpenAI`).\n",
    "* `tools`: La lista de funciones disponibles para el agente.\n",
    "* `name`: Un nombre único para este agente (útil para su identificación en un sistema multiagente).\n",
    "* `prompt`: El prompt del sistema que guía el comportamiento del LLM.\n",
    "* `state_schema`: La clase `State` que definimos, asegurando consistencia entre agentes.\n",
    "* `checkpointer` y `store`: Nuestros mecanismos de memoria para el estado a nivel de conversación y datos del usuario a largo plazo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent # Import the pre-built ReAct agent creator\n",
    "\n",
    "# Define the invoice information subagent using the pre-built `create_react_agent`.\n",
    "# This function internally sets up the nodes (LLM and ToolNode) and edges for a ReAct loop.\n",
    "invoice_information_subagent = create_react_agent(\n",
    "    llm,                          # The language model to use for reasoning\n",
    "    tools=invoice_tools,            # The list of tools available to this agent\n",
    "    name=\"invoice_information_subagent\", # A unique name for this agent within the graph\n",
    "    prompt=invoice_subagent_prompt, # The system prompt for this agent's persona and instructions\n",
    "    state_schema=State,             # The shared state schema for the graph\n",
    "    checkpointer=checkpointer,      # The checkpointer for short-term (thread-level) memory\n",
    "    store = in_memory_store         # The in-memory store for long-term user data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Invoice Information Sub-Agent!\n",
    "\n",
    "Let's test our newly created invoice sub-agent to ensure it correctly processes queries related to invoices and utilizes its specific tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread_id = uuid.uuid4() # Generate a new unique thread ID for this test conversation.\n",
    "\n",
    "# Define a sample question for the invoice sub-agent.\n",
    "question = \"My customer id is 1. What was my most recent invoice, and who was the employee that helped me with it?\"\n",
    "\n",
    "# Set up the configuration with the thread ID.\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Invoke the invoice sub-agent with the question and configuration.\n",
    "result = invoice_information_subagent.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "\n",
    "# Print the conversation history from the result for verification.\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Construcción de una Arquitectura Multiagente (Supervisor)\n",
    "\n",
    "Ahora que tenemos dos subagentes especializados—uno para consultas sobre el catálogo musical y otro para información de facturación—el siguiente paso lógico es garantizar que las consultas de los clientes se dirijan al agente adecuado. Aquí es donde entra en juego un **agente supervisor**.\n",
    "\n",
    "El supervisor actúa como coordinador central, supervisando el flujo de trabajo e invocando de forma inteligente al subagente más relevante según la consulta del cliente. Esto elimina la necesidad de que cada subagente entienda todos los tipos de consultas posibles, permitiéndoles enfocarse en su especialidad.\n",
    "\n",
    "### El Rol del Supervisor\n",
    "\n",
    "![Arquitectura del Supervisor](images/supervisor.png)\n",
    "\n",
    "Las responsabilidades principales del supervisor incluyen:\n",
    "\n",
    "* **Reconocimiento de Intención**: Analiza la consulta del cliente para determinar su intención principal (por ejemplo, relacionada con música, facturación u otra categoría).\n",
    "\n",
    "  * `music_catalog_information_subagent`: Atiende consultas sobre canciones, álbumes, artistas, géneros y preferencias musicales.\n",
    "  * `invoice_information_subagent`: Atiende consultas sobre compras anteriores, facturas y detalles de facturación.\n",
    "* **Enrutamiento**: Dirige la consulta al subagente especializado apropiado según la intención reconocida.\n",
    "* **Orquestación (Implícita)**: Aunque el supervisor realiza el *enrutamiento* de forma explícita, orquesta de forma implícita al asegurar que el agente correcto esté activo en el momento adecuado. En conversaciones de múltiples turnos, puede reenrutar o permitir que el subagente actual continúe.\n",
    "\n",
    "LangGraph ofrece una utilidad preconstruida llamada `create_supervisor`, que simplifica la creación de este mecanismo de enrutamiento. Esta función configura un potente enrutador basado en LLM que aprovecha las descripciones y capacidades de los subagentes para tomar decisiones de enrutamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, crearemos un conjunto de instrucciones para nuestro supervisor. Este prompt define la personalidad del supervisor, su rol como planificador y enrutador, y las capacidades de los subagentes que supervisa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_prompt = \"\"\"You are an expert customer support assistant for a digital music store. \n",
    "You are dedicated to providing exceptional service and ensuring customer queries are answered thoroughly. \n",
    "You have a team of subagents that you can use to help answer queries from customers. \n",
    "Your primary role is to serve as a supervisor/planner for this multi-agent team that helps answer queries from customers. \n",
    "\n",
    "Your team is composed of two subagents that you can use to help answer the customer's request:\n",
    "1. music_catalog_information_subagent: this subagent has access to user's saved music preferences. It can also retrieve information about the digital music store's music \n",
    "catalog (albums, tracks, songs, etc.) from the database. \n",
    "3. invoice_information_subagent: this subagent is able to retrieve information about a customer's past purchases or invoices \n",
    "from the database. \n",
    "\n",
    "Based on the existing steps that have been taken in the messages, your role is to generate the next subagent that needs to be called. \n",
    "This could be one step in an inquiry that needs multiple sub-agent calls. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor # Import the pre-built supervisor creator\n",
    "\n",
    "# Create the supervisor workflow using the `create_supervisor` utility.\n",
    "# This function dynamically sets up the graph to route between the provided agents.\n",
    "supervisor_prebuilt_workflow = create_supervisor(\n",
    "    agents=[invoice_information_subagent, music_catalog_subagent], # List of sub-agents the supervisor can route to\n",
    "    output_mode=\"last_message\", # Specifies that the supervisor should output only the last message from the routed agent.\n",
    "                                # Alternative is \"full_history\" to get all messages from the sub-agent.\n",
    "    model=llm,                # The LLM to act as the supervisor (for routing decisions).\n",
    "    prompt=(supervisor_prompt), # The system prompt guiding the supervisor's behavior.\n",
    "    state_schema=State          # The shared state schema for the entire multi-agent graph.\n",
    ")\n",
    "\n",
    "# Compile the supervisor workflow into a runnable object.\n",
    "# This makes it ready for invocation and integrates it with our memory systems.\n",
    "supervisor_prebuilt = supervisor_prebuilt_workflow.compile(name=\"music_catalog_subagent\", checkpointer=checkpointer, store=in_memory_store)\n",
    "\n",
    "# Display a visualization of the compiled supervisor graph.\n",
    "# Notice how the supervisor acts as the central hub, directing traffic to its sub-agents.\n",
    "show_graph(supervisor_prebuilt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba del Supervisor Multiagente\n",
    "\n",
    "Probemos nuestro sistema multiagente recién compilado con el supervisor en funcionamiento. Proporcionaremos una consulta que pueda involucrar tanto información musical como de facturación, o principalmente un dominio, para observar cómo el supervisor enruta correctamente la solicitud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread_id = uuid.uuid4() # Generate a fresh thread ID for this conversation.\n",
    "\n",
    "# Define a question that involves both invoice and music information.\n",
    "question = \"My customer ID is 1. How much was my most recent purchase? What albums do you have by U2?\"\n",
    "\n",
    "# Configure the invocation with the thread ID.\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Invoke the `supervisor_prebuilt` graph with the human message.\n",
    "# The supervisor will analyze the question, route it to the appropriate sub-agent(s), \n",
    "# and return the final response from the last active agent.\n",
    "result = supervisor_prebuilt.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "\n",
    "# Print the messages from the resulting state to see the conversation flow and final answer.\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Añadiendo Verificación del Cliente mediante Human-in-the-Loop\n",
    "\n",
    "Actualmente, nuestro agente asume que tenemos el `customer_id` disponible o proporcionado directamente en la consulta inicial. En un escenario realista de soporte al cliente, a menudo necesitamos **primero verificar la identidad del cliente** antes de procesar consultas sensibles como detalles de facturación. Esta verificación puede implicar pedir al usuario un identificador (por ejemplo, correo electrónico, teléfono, ID de cliente) y luego buscarlo en la base de datos.\n",
    "\n",
    "Para implementar esto, introduciremos un componente de **human-in-the-loop**. Esto significa que el grafo puede *pausar* la ejecución y esperar una entrada adicional del usuario (o de un agente humano) antes de continuar. El mecanismo `interrupt` de LangGraph es perfecto para esto.\n",
    "\n",
    "![Integración de Entrada Humana](images/human_input.png)\n",
    "\n",
    "En este paso, añadiremos dos nuevos nodos a nuestro flujo de trabajo:\n",
    "\n",
    "* **Nodo `verify_info`**: Este nodo intentará extraer un identificador del cliente a partir del input del usuario y verificarlo contra nuestra base de datos. Si lo encuentra, actualiza el `customer_id` en el estado del grafo. Si no se encuentra o no se proporciona, solicita al usuario la información.\n",
    "* **Nodo `human_input`**: Es un nodo simple que activa explícitamente una `interrupt`, pausando la ejecución del grafo hasta que el usuario proporcione la información necesaria para continuar.\n",
    "\n",
    "También aprovecharemos las capacidades de salida estructurada de LangChain para analizar confiablemente la entrada del usuario en busca de identificadores usando esquemas Pydantic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field # Pydantic for defining data schemas and field validation\n",
    "\n",
    "# Define a Pydantic BaseModel to structure the expected user input for account information.\n",
    "# This helps the LLM to parse specific entities (identifier) from free-form text.\n",
    "class UserInput(BaseModel):\n",
    "    \"\"\"Schema for parsing user-provided account information.\"\"\"\n",
    "    # `identifier` field: Expects a string, with a description for the LLM to understand its purpose.\n",
    "    identifier: str = Field(description = \"Identifier, which can be a customer ID, email, or phone number.\")\n",
    "\n",
    "# Bind the Pydantic schema to our LLM using `with_structured_output`.\n",
    "# This forces the LLM to generate output that conforms to the `UserInput` schema, making parsing reliable.\n",
    "structured_llm = llm.with_structured_output(schema=UserInput)\n",
    "\n",
    "# Define a system prompt specifically for the structured LLM.\n",
    "# This prompt instructs the LLM on how to extract the customer identifier from messages.\n",
    "structured_system_prompt = \"\"\"You are a customer service representative responsible for extracting customer identifier.\\n \n",
    "Only extract the customer's account information from the message history. \n",
    "If they haven't provided the information yet, return an empty string for the file\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional # For type hinting optional values\n",
    "\n",
    "# Helper function to retrieve a customer ID from various identifiers (ID, phone, email).\n",
    "def get_customer_id_from_identifier(identifier: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Retrieve Customer ID using an identifier, which can be a customer ID, email, or phone number.\n",
    "    \n",
    "    Args:\n",
    "        identifier (str): The identifier can be customer ID, email, or phone.\n",
    "    \n",
    "    Returns:\n",
    "        Optional[int]: The CustomerId if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Check if the identifier is purely numeric, indicating a direct customer ID.\n",
    "    if identifier.isdigit():\n",
    "        return int(identifier)\n",
    "    \n",
    "    # Check if the identifier starts with '+', suggesting a phone number.\n",
    "    elif identifier[0] == \"+\":\n",
    "        query = f\"SELECT CustomerId FROM Customer WHERE Phone = '{identifier}';\"\n",
    "        result = db.run(query)\n",
    "        formatted_result = ast.literal_eval(result) # Safely evaluate string to list/tuple\n",
    "        if formatted_result:\n",
    "            return formatted_result[0][0] # Return the first CustomerId found\n",
    "    \n",
    "    # Check if the identifier contains '@', suggesting an email address.\n",
    "    elif \"@\" in identifier:\n",
    "        query = f\"SELECT CustomerId FROM Customer WHERE Email = '{identifier}';\"\n",
    "        result = db.run(query)\n",
    "        formatted_result = ast.literal_eval(result)\n",
    "        if formatted_result:\n",
    "            return formatted_result[0][0] # Return the first CustomerId found\n",
    "    \n",
    "    # If no matching identifier type is found or no ID is retrieved, return None.\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `verify_info` node function.\n",
    "# This node is responsible for verifying the customer's identity based on their input.\n",
    "def verify_info(state: State, config: RunnableConfig):\n",
    "    \"\"\"Verify the customer's account by parsing their input and matching it with the database.\"\"\"\n",
    "\n",
    "    # Check if a customer_id is already present in the state.\n",
    "    # If it is, verification is complete, and the node does nothing (passes).\n",
    "    if state.get(\"customer_id\") is None: \n",
    "        # System instructions for the verification LLM.\n",
    "        system_instructions = \"\"\"You are a music store agent, where you are trying to verify the customer identity \n",
    "        as the first step of the customer support process. \n",
    "        Only after their account is verified, you would be able to support them on resolving the issue. \n",
    "        In order to verify their identity, one of their customer ID, email, or phone number needs to be provided.\n",
    "        If the customer has not provided the information yet, please ask them for it.\n",
    "        If they have provided the identifier but cannot be found, please ask them to revise it.\"\"\"\n",
    "\n",
    "        # Get the most recent user message from the state.\n",
    "        user_input = state[\"messages\"][-1] \n",
    "    \n",
    "        # Use the structured LLM to parse the user's input for an identifier.\n",
    "        # It combines the structured system prompt with the user's message.\n",
    "        parsed_info = structured_llm.invoke([SystemMessage(content=structured_system_prompt)] + [user_input])\n",
    "    \n",
    "        # Extract the identified identifier string.\n",
    "        identifier = parsed_info.identifier\n",
    "    \n",
    "        customer_id = \"\" # Initialize customer_id as an empty string.\n",
    "        # Attempt to find the customer ID in the database using the helper function.\n",
    "        if (identifier):\n",
    "            customer_id = get_customer_id_from_identifier(identifier)\n",
    "    \n",
    "        # If a valid customer_id was found,\n",
    "        if customer_id != \"\":\n",
    "            # Create a system message confirming verification.\n",
    "            intent_message = SystemMessage(\n",
    "                content= f\"Thank you for providing your information! I was able to verify your account with customer id {customer_id}.\"\n",
    "            )\n",
    "            # Update the state with the found customer_id and the confirmation message.\n",
    "            return {\n",
    "                  \"customer_id\": customer_id,\n",
    "                  \"messages\" : [intent_message]\n",
    "                  }\n",
    "        # If no customer_id was found or provided,\n",
    "        else:\n",
    "          # Invoke the base LLM with instructions to prompt the user for their identifier or revise it.\n",
    "          response = llm.invoke([SystemMessage(content=system_instructions)]+state['messages'])\n",
    "          # Update the state with the LLM's response (the prompt for user input).\n",
    "          return {\"messages\": [response]}\n",
    "\n",
    "    else: \n",
    "        # If `customer_id` is already in state, this node does nothing.\n",
    "        # This `pass` implies that the graph will simply proceed to the next edge, \n",
    "        # as defined in the graph compilation.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a crear nuestro nodo `human_input`. Este nodo está diseñado para activar una `interrupt` en el grafo. Cuando ocurre una interrupción, el grafo se pausa y el control regresa al llamador (por ejemplo, el notebook o una aplicación). El llamador puede entonces elegir reanudar el grafo, proporcionando opcionalmente nueva información.\n",
    "\n",
    "Así es como implementamos la interacción human-in-the-loop para la verificación del cliente: el grafo solicita un identificador, se pausa y espera a que el usuario lo proporcione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt # Import the `interrupt` function for pausing graph execution\n",
    "\n",
    "# Define the `human_input` node function.\n",
    "# This node serves as a placeholder to signal that human intervention is required.\n",
    "def human_input(state: State, config: RunnableConfig):\n",
    "    \"\"\" No-op node that should be interrupted on \"\"\"\n",
    "    # `interrupt(\"Please provide input.\")` pauses the graph execution.\n",
    "    # The string message is passed as a reason for the interrupt.\n",
    "    # When the graph is resumed, the new input will be stored in `user_input`.\n",
    "    user_input = interrupt(\"Please provide input.\")\n",
    "    \n",
    "    # The new user input (after resume) is then added to the messages in the state.\n",
    "    return {\"messages\": [user_input]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Vamos a juntar todo esto! Integraremos los nodos `verify_info` y `human_input` en una nueva estructura de grafo. El flujo será el siguiente:\n",
    "\n",
    "1. **`START`** -> **`verify_info`**: Todas las consultas entrantes intentan primero verificar al cliente.\n",
    "2. **`verify_info` (Bifurcación condicional)**:\n",
    "\n",
    "   * Si *no se encuentra* el `customer_id` (lo que significa que la verificación falló o que falta entrada), se redirige a **`human_input`**.\n",
    "   * Si *se encuentra* el `customer_id` (es decir, verificación exitosa), se redirige al **`supervisor`**.\n",
    "3. **`human_input`** -> **`verify_info`**: Después de que el usuario proporciona la información para reanudar el grafo, se vuelve a intentar la verificación pasando nuevamente por `verify_info`.\n",
    "4. **`supervisor`** -> **`END`**: Una vez que la consulta principal es atendida por el supervisor y sus subagentes, el grafo finaliza.\n",
    "\n",
    "Esta configuración garantiza que la identidad del cliente sea verificada antes de cualquier otra acción, y maneja de forma elegante los casos en que la identidad necesita ser proporcionada o reintentada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditional edge function for `verify_info`.\n",
    "# This function checks if a `customer_id` has been successfully set in the state.\n",
    "def should_interrupt(state: State, config: RunnableConfig):\n",
    "    # If `customer_id` is present, it means verification was successful or already done, so continue.\n",
    "    if state.get(\"customer_id\") is not None:\n",
    "        return \"continue\"\n",
    "    # Otherwise, it means customer ID is missing or couldn't be verified, so interrupt for human input.\n",
    "    else:\n",
    "        return \"interrupt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new StateGraph for our multi-agent system with human-in-the-loop verification.\n",
    "multi_agent_verify = StateGraph(State)\n",
    "\n",
    "# Add the `verify_info` node for customer identity verification.\n",
    "multi_agent_verify.add_node(\"verify_info\", verify_info)\n",
    "\n",
    "# Add the `human_input` node, which triggers an interrupt to get user input.\n",
    "multi_agent_verify.add_node(\"human_input\", human_input)\n",
    "\n",
    "# Add the `supervisor` node, which orchestrates the sub-agents for query handling.\n",
    "multi_agent_verify.add_node(\"supervisor\", supervisor_prebuilt)\n",
    "\n",
    "# Define the entry point: all interactions start with customer verification.\n",
    "multi_agent_verify.add_edge(START, \"verify_info\")\n",
    "\n",
    "# Define the conditional routing after `verify_info`.\n",
    "# `should_interrupt` decides whether to continue to the supervisor or prompt for human input.\n",
    "multi_agent_verify.add_conditional_edges(\n",
    "    \"verify_info\",     # Source node\n",
    "    should_interrupt,  # Conditional function\n",
    "    {\n",
    "        # If verification is successful, continue to the main supervisor agent.\n",
    "        \"continue\": \"supervisor\",\n",
    "        # If verification is needed (or failed), route to `human_input` to prompt the user.\n",
    "        \"interrupt\": \"human_input\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# After `human_input` (once resumed), loop back to `verify_info` to try verification again.\n",
    "multi_agent_verify.add_edge(\"human_input\", \"verify_info\")\n",
    "\n",
    "# The supervisor is the final processing stage before the graph ends.\n",
    "multi_agent_verify.add_edge(\"supervisor\", END)\n",
    "\n",
    "# Compile the complete graph, integrating all nodes and edges with our memory systems.\n",
    "multi_agent_verify_graph = multi_agent_verify.compile(name=\"multi_agent_verify\", checkpointer=checkpointer, store=in_memory_store)\n",
    "\n",
    "# Display the visualization of the new graph.\n",
    "show_graph(multi_agent_verify_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probando la Verificación Human-in-the-Loop\n",
    "\n",
    "Vamos a probar nuestro grafo actualizado. Comenzaremos con una pregunta que requiere un ID de cliente, pero sin proporcionarlo inicialmente. Esto debería activar el nodo `human_input` y pausar la ejecución.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = uuid.uuid4() # Generate a new unique thread ID.\n",
    "\n",
    "# Initial question without providing customer ID.\n",
    "question = \"How much was my most recent purchase?\"\n",
    "\n",
    "# Configuration for the graph invocation.\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Invoke the graph. This first invocation should hit the `human_input` node and interrupt.\n",
    "result = multi_agent_verify_graph.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "\n",
    "# Print messages to observe the agent asking for the customer ID.\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langgraph.types import Command # Import Command for resuming graph execution\n",
    "\n",
    "# Now, we simulate the user providing their phone number to resume the conversation.\n",
    "question = \"My phone number is +55 (12) 3923-5555.\"\n",
    "\n",
    "# Resume from the interrupt using `Command(resume=...)`. \n",
    "# The `resume` argument carries the new user input, which gets processed by `human_input` node \n",
    "# and then passed back to `verify_info`.\n",
    "# The `config` must be the same as the initial invocation to resume the correct thread.\n",
    "result = multi_agent_verify_graph.invoke(Command(resume=question), config=config)\n",
    "\n",
    "# Print the conversation messages to see the verification and subsequent processing.\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, si hacemos una pregunta de seguimiento dentro del *mismo hilo*, el estado de nuestro agente (gestionado por el checkpointer) ya *almacenará nuestro `customer_id`*. Esto significa que el nodo `verify_info` simplemente hará `pass` sin volver a pedir la información, y la consulta se enviará directamente al supervisor, demostrando el beneficio de la memoria a corto plazo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Follow-up question in the same thread (using the same `thread_id`).\n",
    "question = \"What albums do you have by the Rolling Stones?\"\n",
    "\n",
    "# Invoke the graph again. Since the `customer_id` is already in the state,\n",
    "# the verification step will be skipped, and the query will directly go to the supervisor.\n",
    "result = multi_agent_verify_graph.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "\n",
    "# Print the results. You should see the music catalog sub-agent's response directly.\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Añadiendo Memoria a Largo Plazo (Preferencias del Usuario)\n",
    "\n",
    "Hemos construido con éxito un flujo de trabajo de agente que maneja la verificación y la ejecución multiagente. Vamos a mejorarlo aún más integrando la **memoria a largo plazo**. Mientras que la memoria a corto plazo (mediante `checkpointers`) mantiene el contexto dentro de una sola conversación, la memoria a largo plazo permite que nuestro agente almacene y recupere información *entre conversaciones* o a través de diferentes sesiones para el mismo usuario. Esto es crucial para la personalización.\n",
    "\n",
    "![Integración de Memoria](images/memory.png)\n",
    "\n",
    "En este paso, añadiremos dos nuevos nodos para gestionar las preferencias del usuario relacionadas con la música:\n",
    "\n",
    "* **Nodo `load_memory`**: Este nodo cargará cualquier preferencia musical existente asociada con el `customer_id` verificado desde nuestro `InMemoryStore` (nuestra memoria a largo plazo) al `State` actual del grafo (`loaded_memory`). Esto garantiza que el agente tenga el contexto relevante para generar respuestas personalizadas.\n",
    "\n",
    "* **Nodo `create_memory`**: Después de que se procese la consulta principal, este nodo analizará la conversación que acaba de tener lugar. Si el cliente compartió algún nuevo interés musical, actualizará o creará un `UserProfile` en el `InMemoryStore`, guardando estas preferencias para futuras interacciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.base import BaseStore # Base class for defining custom stores for LangGraph\n",
    "\n",
    "# Helper function to format user memory (music preferences) into a readable string.\n",
    "def format_user_memory(user_data):\n",
    "    \"\"\"Formats music preferences from users, if available.\"\"\"\n",
    "    profile = user_data['memory'] # Access the 'memory' key from the stored dictionary\n",
    "    result = \"\" # Initialize an empty string for the formatted result\n",
    "    \n",
    "    # Check if the profile object has a 'music_preferences' attribute and if it's not empty.\n",
    "    if hasattr(profile, 'music_preferences') and profile.music_preferences:\n",
    "        # If preferences exist, join them into a comma-separated string.\n",
    "        result += f\"Music Preferences: {', '.join(profile.music_preferences)}\"\n",
    "    \n",
    "    return result.strip() # Return the formatted string, removing any leading/trailing whitespace.\n",
    "\n",
    "# Define the `load_memory` node function.\n",
    "# This node loads a user's long-term memory (music preferences) into the current state.\n",
    "def load_memory(state: State, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Loads music preferences from users, if available.\"\"\"\n",
    "    \n",
    "    user_id = state[\"customer_id\"] # Get the current customer ID from the state.\n",
    "    namespace = (\"memory_profile\", user_id) # Define a namespace for storing user-specific memory.\n",
    "                                          # This creates a unique key for each user's profile.\n",
    "    \n",
    "    # Attempt to retrieve existing memory for this user from the `InMemoryStore`.\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "    \n",
    "    formatted_memory = \"\" # Initialize formatted memory as empty.\n",
    "    \n",
    "    # If memory exists and has a value, format it using our helper function.\n",
    "    if existing_memory and existing_memory.value:\n",
    "        formatted_memory = format_user_memory(existing_memory.value)\n",
    "\n",
    "    # Update the `loaded_memory` field in the state with the retrieved and formatted memory.\n",
    "    return {\"loaded_memory\" : formatted_memory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic BaseModel to structure the `UserProfile` for long-term memory.\n",
    "# This ensures that user preferences are stored in a consistent and verifiable format.\n",
    "class UserProfile(BaseModel):\n",
    "    # `customer_id`: Required field for the customer's unique identifier.\n",
    "    customer_id: str = Field(\n",
    "        description=\"The customer ID of the customer\"\n",
    "    )\n",
    "    # `music_preferences`: A list of strings to store the customer's music interests.\n",
    "    music_preferences: List[str] = Field(\n",
    "        description=\"The music preferences of the customer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the `create_memory` LLM.\n",
    "# This prompt instructs an LLM to act as an analyst, analyzing conversation history\n",
    "# to extract and update user music preferences in a structured `UserProfile` format.\n",
    "create_memory_prompt = \"\"\"You are an expert analyst that is observing a conversation that has taken place between a customer and a customer support assistant. The customer support assistant works for a digital music store, and has utilized a multi-agent team to answer the customer's request. \n",
    "You are tasked with analyzing the conversation that has taken place between the customer and the customer support assistant, and updating the memory profile associated with the customer. The memory profile may be empty. If it's empty, you should create a new memory profile for the customer.\n",
    "\n",
    "You specifically care about saving any music interest the customer has shared about themselves, particularly their music preferences to their memory profile.\n",
    "\n",
    "To help you with this task, I have attached the conversation that has taken place between the customer and the customer support assistant below, as well as the existing memory profile associated with the customer that you should either update or create. \n",
    "\n",
    "The customer's memory profile should have the following fields:\n",
    "- customer_id: the customer ID of the customer\n",
    "- music_preferences: the music preferences of the customer\n",
    "\n",
    "These are the fields you should keep track of and update in the memory profile. If there has been no new information shared by the customer, you should not update the memory profile. It is completely okay if you do not have new information to update the memory profile with. In that case, just leave the values as they are.\n",
    "\n",
    "*IMPORTANT INFORMATION BELOW*\n",
    "\n",
    "The conversation between the customer and the customer support assistant that you should analyze is as follows:\n",
    "{conversation}\n",
    "\n",
    "The existing memory profile associated with the customer that you should either update or create based on the conversation is as follows:\n",
    "{memory_profile}\n",
    "\n",
    "Ensure your response is an object that has the following fields:\n",
    "- customer_id: the customer ID of the customer\n",
    "- music_preferences: the music preferences of the customer\n",
    "\n",
    "For each key in the object, if there is no new information, do not update the value, just keep the value that is already there. If there is new information, update the value. \n",
    "\n",
    "Take a deep breath and think carefully before responding.\n",
    "\"\"\"\n",
    "\n",
    "# Define the `create_memory` node function.\n",
    "# This node is responsible for analyzing the conversation and saving/updating user music preferences.\n",
    "def create_memory(state: State, config: RunnableConfig, store: BaseStore):\n",
    "    user_id = str(state[\"customer_id\"]) # Get the customer ID from the current state (convert to string).\n",
    "    namespace = (\"memory_profile\", user_id) # Define the namespace for this user's memory profile.\n",
    "    \n",
    "    # Retrieve the existing memory profile for this user from the long-term store.\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "    \n",
    "    formatted_memory = \"\" # Initialize formatted memory for the prompt.\n",
    "    if existing_memory and existing_memory.value:\n",
    "        existing_memory_dict = existing_memory.value # Get the dictionary containing the UserProfile instance.\n",
    "        # Format existing music preferences into a string for the prompt.\n",
    "        formatted_memory = (\n",
    "            f\"Music Preferences: {', '.join(existing_memory_dict.get('memory').music_preferences or [])}\" # Access the UserProfile object via 'memory' key\n",
    "        )\n",
    "\n",
    "    # Create a SystemMessage with the formatted prompt, injecting the full conversation history\n",
    "# and the existing memory profile.\n",
    "    formatted_system_message = SystemMessage(content=create_memory_prompt.format(conversation=state[\"messages\"], memory_profile=formatted_memory))\n",
    "    \n",
    "    # Invoke the LLM with structured output (`UserProfile`) to analyze the conversation\n",
    "    # and update the memory profile based on new information.\n",
    "    updated_memory = llm.with_structured_output(UserProfile).invoke([formatted_system_message])\n",
    "    \n",
    "    key = \"user_memory\" # Define the key for storing this specific memory object.\n",
    "    \n",
    "    # Store the updated memory profile back into the `InMemoryStore`.\n",
    "    # We wrap `updated_memory` in a dictionary under the key 'memory' for consistency in access.\n",
    "    store.put(namespace, key, {\"memory\": updated_memory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the final StateGraph for our complete multi-agent system, including memory management.\n",
    "multi_agent_final = StateGraph(State)\n",
    "\n",
    "# Add all necessary nodes to the graph.\n",
    "multi_agent_final.add_node(\"verify_info\", verify_info)         # Node for customer verification\n",
    "multi_agent_final.add_node(\"human_input\", human_input)         # Node for human-in-the-loop interruption\n",
    "multi_agent_final.add_node(\"load_memory\", load_memory)         # Node for loading user long-term memory\n",
    "multi_agent_final.add_node(\"supervisor\", supervisor_prebuilt) # Supervisor for routing to sub-agents\n",
    "multi_agent_final.add_node(\"create_memory\", create_memory)     # Node for saving/updating user long-term memory\n",
    "\n",
    "# Define the initial entry point: all interactions start with verification.\n",
    "multi_agent_final.add_edge(START, \"verify_info\")\n",
    "\n",
    "# Define the conditional routing after `verify_info`.\n",
    "# If verification is successful, proceed to load memory. Otherwise, prompt for human input.\n",
    "multi_agent_final.add_conditional_edges(\n",
    "    \"verify_info\",\n",
    "    should_interrupt,\n",
    "    {\n",
    "        \"continue\": \"load_memory\", # If verified, load user memory\n",
    "        \"interrupt\": \"human_input\", # If not verified, request human input\n",
    "    },\n",
    ")\n",
    "\n",
    "# After `human_input` (resume), loop back to `verify_info` to re-attempt verification.\n",
    "multi_agent_final.add_edge(\"human_input\", \"verify_info\")\n",
    "\n",
    "# After loading memory, proceed to the supervisor for main query processing.\n",
    "multi_agent_final.add_edge(\"load_memory\", \"supervisor\")\n",
    "\n",
    "# After the supervisor completes, save/update the user's memory.\n",
    "multi_agent_final.add_edge(\"supervisor\", \"create_memory\")\n",
    "\n",
    "# The graph ends after memory has been updated.\n",
    "multi_agent_final.add_edge(\"create_memory\", END)\n",
    "\n",
    "# Compile the entire, sophisticated graph.\n",
    "multi_agent_final_graph = multi_agent_final.compile(name=\"multi_agent_verify\", checkpointer=checkpointer, store=in_memory_store)\n",
    "\n",
    "# Display the visualization of the complete graph.\n",
    "show_graph(multi_agent_final_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Vamos a probarlo! Usaremos una consulta compleja que requiere verificación, luego aborda tanto información musical como de facturación, y además incluye una preferencia musical que debería guardarse en la memoria a largo plazo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread_id = uuid.uuid4() # Generate a fresh unique thread ID for this demonstration.\n",
    "\n",
    "# A comprehensive question that includes customer ID, invoice query, and music preference.\n",
    "question = \"My phone number is +55 (12) 3923-5555. How much was my most recent purchase? What albums do you have by the Rolling Stones?\"\n",
    "\n",
    "# Configuration for the graph invocation.\n",
    "# Note: The user_id is passed as a configurable parameter, although in this specific example,\n",
    "# the customer_id is extracted dynamically by the verify_info node. \n",
    "# For real-world use, ensure consistent handling of user identifiers.\n",
    "config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"1\"}}\n",
    "\n",
    "# Invoke the final multi-agent graph.\n",
    "# This will run through verification, memory loading, supervisor routing (to invoice then music),\n",
    "# and finally memory saving.\n",
    "result = multi_agent_final_graph.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "\n",
    "# Print all messages in the final state to observe the complete interaction flow.\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a la memoria. Esperamos que las preferencias musicales (por ejemplo, \"Rolling Stones\") se hayan guardado en nuestro `in_memory_store` bajo el ID de cliente asociado con el número de teléfono `+55 (12) 3923-5555` (que corresponde al ID de cliente 1 en nuestra base de datos Chinook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_id = \"1\" # The customer ID we expect to be associated with the phone number used.\n",
    "namespace = (\"memory_profile\", user_id) # The namespace used to store this user's memory.\n",
    "\n",
    "# Retrieve the user's memory profile from the `in_memory_store`.\n",
    "# `.value` retrieves the actual data stored, which should be a dictionary containing the UserProfile instance.\n",
    "memory_data = in_memory_store.get(namespace, \"user_memory\")\n",
    "\n",
    "# Check if memory_data exists and has a 'memory' key (which holds the UserProfile object).\n",
    "if memory_data and \"memory\" in memory_data.value:\n",
    "    saved_music_preferences = memory_data.value.get(\"memory\").music_preferences\n",
    "else:\n",
    "    saved_music_preferences = [] # Default to empty list if no preferences found.\n",
    "\n",
    "print(f\"Saved Music Preferences for Customer ID {user_id}: {saved_music_preferences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Opcional) Construir un Gráfico Multi-Agente Tipo Swarm\n",
    "\n",
    "Más allá del patrón centralizado con supervisor, otro enfoque poderoso para sistemas multi-agente es la **arquitectura Swarm**. Mientras que el supervisor depende de un agente central que orquesta todo, el modelo swarm enfatiza la colaboración descentralizada.\n",
    "\n",
    "### Arquitectura Swarm\n",
    "\n",
    "![Arquitectura Swarm](images/swarm.png)\n",
    "\n",
    "En un swarm, múltiples agentes especializados trabajan juntos sin un coordinador central. Cada agente conoce las capacidades de los demás y puede *transferir directamente* una tarea al par más adecuado cuando su propia experiencia no es suficiente o cuando una consulta cruza dominios. Esto crea un flujo dinámico y flexible donde los agentes se pasan el control entre sí según sea necesario.\n",
    "\n",
    "En LangChain, hemos creado una [librería ligera](https://github.com/langchain-ai/langgraph-swarm-py) para ayudar a crear agentes Swarm fácilmente con LangGraph.\n",
    "\n",
    "### Swarm vs Supervisor\n",
    "\n",
    "![Comparación Swarm vs Supervisor](images/supervisor_vs_swarm.png)\n",
    "\n",
    "Recapitulemos las diferencias clave:\n",
    "\n",
    "| Característica         | Arquitectura con Supervisor                      | Arquitectura Swarm                                 |\n",
    "| :--------------------- | :----------------------------------------------- | :------------------------------------------------- |\n",
    "| **Flujo de control**   | Centralizado; el supervisor enruta consultas.    | Descentralizado; los agentes se pasan tareas.      |\n",
    "| **Toma de decisiones** | El supervisor decide qué agente llamar.          | Cada agente decide a quién pasar el control.       |\n",
    "| **Jerarquía**          | Jerarquía clara: Supervisor -> Sub-agentes.      | Estructura plana y colaborativa.                   |\n",
    "| **Modularidad**        | Añadir/quitar agentes modificando el supervisor. | Los agentes deben conocer posibles transferencias. |\n",
    "| **Previsibilidad**     | Enrutamiento más predecible.                     | Comportamiento dinámico y emergente.               |\n",
    "| **Resiliencia**        | El supervisor es un punto único de falla.        | Más resiliente a fallas individuales.              |\n",
    "\n",
    "La elección entre supervisor y swarm depende del caso de uso. Si necesitas control estricto y un flujo claro, el supervisor suele ser la mejor opción. Si tu problema se beneficia de una resolución de problemas colaborativa y dinámica, un swarm puede ser más adecuado.\n",
    "\n",
    "Para más información, hay un excelente video de Lance, del equipo de LangChain, que explica en detalle: [Multi-agent swarms with LangGraph](https://www.youtube.com/watch?v=JeyDrn1dSUQ)\n",
    "\n",
    "¡Vamos a crear agentes swarm!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_swarm import create_handoff_tool, create_swarm # Import utilities for creating swarm agents and handoff tools\n",
    "\n",
    "# Create our handoff tools between agents.\n",
    "# These are special tools that, when called by an agent, signal a transfer of control\n",
    "# to another named agent within the swarm.\n",
    "\n",
    "transfer_to_invoice_agent_handoff_tool = create_handoff_tool(\n",
    "    agent_name = \"invoice_information_agent_with_handoff\", # The name of the target agent for this handoff\n",
    "    description = \"Transfer user to the invoice information agent that can help with invoice information\" # Description for LLM\n",
    ")\n",
    "\n",
    "transfer_to_music_catalog_agent_handoff_tool = create_handoff_tool(\n",
    "    agent_name = \"music_catalog_agent_with_handoff\", \n",
    "    description = \"Transfer user to the music catalog agent that can help with music searches and music catalog information\"\n",
    ")\n",
    "\n",
    "# Recreate our agents, but this time, add the handoff tools to their available tools.\n",
    "# This allows each agent to `request` a handoff to the other when appropriate.\n",
    "\n",
    "# First, combine the handoff tools with the existing specific tools for each agent.\n",
    "invoice_tools_with_handoff = [transfer_to_music_catalog_agent_handoff_tool] + invoice_tools\n",
    "music_tools_with_handoff = [transfer_to_invoice_agent_handoff_tool] + music_tools\n",
    "\n",
    "# Recreate the invoice information agent with its original prompt and its new set of tools (including handoff).\n",
    "invoice_information_agent_with_handoff = create_react_agent(\n",
    "    llm,\n",
    "    invoice_tools_with_handoff,\n",
    "    prompt = invoice_subagent_prompt,\n",
    "    name = \"invoice_information_agent_with_handoff\" # Give it a specific name for the swarm\n",
    ")\n",
    "\n",
    "# Recreate the music catalog agent with its original prompt and its new set of tools (including handoff).\n",
    "# Note: The music catalog agent prompt is generated dynamically, as defined earlier.\n",
    "music_catalog_agent_with_handoff = create_react_agent(\n",
    "    llm,\n",
    "    music_tools_with_handoff,\n",
    "    prompt = generate_music_assistant_prompt(),\n",
    "    name = \"music_catalog_agent_with_handoff\" # Give it a specific name for the swarm\n",
    ")\n",
    "\n",
    "\n",
    "# Create the swarm workflow. `create_swarm` handles the orchestration logic\n",
    "# for agents to hand off to each other without a central supervisor.\n",
    "swarm_workflow = create_swarm(\n",
    "    agents = [invoice_information_agent_with_handoff, music_catalog_agent_with_handoff], # The agents participating in the swarm\n",
    "    default_active_agent = \"invoice_information_agent_with_handoff\", # The agent that will be active first by default\n",
    ")\n",
    "\n",
    "# Compile the swarm graph. This makes it runnable and integrates memory.\n",
    "swarm_agents = swarm_workflow.compile(\n",
    "    checkpointer = checkpointer,\n",
    "    store = in_memory_store\n",
    ")\n",
    "\n",
    "# Display the graph of the swarm. Notice it's different from the supervisor graph,\n",
    "# showing connections for potential handoffs rather than a central hub.\n",
    "show_graph(swarm_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ¡probémoslo! Le daremos una pregunta relacionada con música, pero como el `default_active_agent` está configurado como el agente de facturas, debería *transferir* la tarea al agente del catálogo musical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new thread for this swarm test.\n",
    "thread_id = uuid.uuid4()\n",
    "\n",
    "# Ask a music-related question.\n",
    "question = \"Do you have any albums by the Rolling Stones?\"\n",
    "\n",
    "# Configure the invocation with the thread ID.\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Invoke the swarm agents. Even though the default active agent is `invoice_information_agent_with_handoff`,\n",
    "# it should recognize that the query is for music and hand off to `music_catalog_agent_with_handoff`.\n",
    "result = swarm_agents.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "\n",
    "# Print the messages to observe the handoff and the final response.\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "\n",
    "**Evaluations** son una forma cuantitativa y sistemática de medir el rendimiento de tus aplicaciones con LLM, especialmente agentes. Son fundamentales porque los LLM no siempre se comportan de manera predecible — incluso pequeños cambios en los prompts, modelos o entradas pueden afectar significativamente los resultados. Las evaluaciones ofrecen un método estructurado para:\n",
    "\n",
    "1. **Identificar Fallos**: Detectar dónde y por qué tu agente no está funcionando como se espera.\n",
    "2. **Comparar Versiones**: Comparar cuantitativamente diferentes versiones de tu aplicación (por ejemplo, tras cambios en el prompt, actualizaciones del modelo o cambios arquitectónicos).\n",
    "3. **Construir Fiabilidad**: Mejorar iterativamente la calidad de tu agente y asegurar que cumple con los estándares de rendimiento deseados.\n",
    "\n",
    "Una evaluación típicamente incluye tres componentes principales:\n",
    "\n",
    "1. **Un Conjunto de Datos**: Una colección de entradas de prueba y sus salidas esperadas (ground truth). Esto sirve como referencia para medir el rendimiento de tu aplicación.\n",
    "2. **Una Aplicación o Función Objetivo**: La parte específica de tu aplicación con LLM (por ejemplo, un agente, cadena o nodo) que deseas evaluar. Esta función toma una entrada y devuelve una salida.\n",
    "3. **Evaluadores**: Métricas o modelos (a menudo LLMs actuando como \"jueces\") que puntúan las salidas de tu función objetivo comparándolas con la ground truth o con criterios específicos.\n",
    "\n",
    "![Evaluation Conceptual Diagram](images/evals-conceptual.png)\n",
    "\n",
    "Existen muchas formas de evaluar un agente, dependiendo del aspecto de su rendimiento que se quiera medir. Hoy veremos tres tipos comunes de evaluaciones para agentes:\n",
    "\n",
    "1. **Evaluación de la Respuesta Final**: Evalúa la calidad de la respuesta final del agente ante una consulta, tratándolo como una caja negra.\n",
    "2. **Evaluación de Paso Único**: Se centra en el rendimiento de un paso específico y crítico durante la ejecución del agente (por ejemplo, si seleccionó correctamente una herramienta).\n",
    "3. **Evaluación de la Trayectoria**: Analiza toda la secuencia de pasos (la \"trayectoria\") que toma un agente, evaluando si sigue el camino esperado de llamadas a herramientas y razonamiento interno.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating The Final Response\n",
    "\n",
    "Evaluar la respuesta final es la forma más común y a menudo la más simple de medir la efectividad general de un agente. Consiste en tratar al agente como una \"caja negra\" y centrarse únicamente en si la salida final responde con éxito a la intención del usuario, sin importar los pasos intermedios que haya tomado.\n",
    "\n",
    "* **Entrada**: La consulta original del usuario.\n",
    "* **Salida**: La respuesta final del agente, dirigida al usuario.\n",
    "\n",
    "![Final Response Evaluation](images/final-response.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Crear un conjunto de datos para la evaluación de la respuesta final\n",
    "\n",
    "Usaremos LangSmith para crear y gestionar nuestros conjuntos de datos de evaluación. Un conjunto de datos consta de `inputs` (las preguntas de los usuarios) y `outputs` (las respuestas correctas esperadas). Esta verdad de base permite a los evaluadores comparar la respuesta generada por el agente con lo que se considera correcto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the LangSmith Client for dataset and experiment management\n",
    "\n",
    "client = Client() # Initialize the LangSmith client. This will connect to your LangSmith account.\n",
    "\n",
    "# Define a list of example inputs and expected outputs for our dataset.\n",
    "# Each dictionary represents a test case with a 'question' (input) and a 'response' (ground truth output).\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"My name is Aaron Mitchell. My number associated with my account is +1 (204) 452-6452. I am trying to find the invoice number for my most recent song purchase. Could you help me with it?\",\n",
    "        \"response\": \"The Invoice ID of your most recent purchase was 342.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I'd like a refund.\",\n",
    "        \"response\": \"I need additional information to help you with the refund. Could you please provide your customer identifier so that we can fetch your purchase history?\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who recorded Wish You Were Here again?\",\n",
    "        \"response\": \"Wish You Were Here is an album by Pink Floyd\",\n",
    "    },\n",
    "    { \n",
    "        \"question\": \"What albums do you have by Coldplay?\",\n",
    "        \"response\": \"There are no Coldplay albums available in our catalog at the moment.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset_name = \"LangGraph 101 Multi-Agent: Final Response\" # Define a name for our dataset.\n",
    "\n",
    "# Check if the dataset already exists in LangSmith to avoid recreation.\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    # If not, create the dataset.\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    # Populate the dataset with our examples.\n",
    "    # `inputs` are extracted from the 'question' key, `outputs` from the 'response' key.\n",
    "    client.create_examples(\n",
    "        inputs=[{\"question\": ex[\"question\"]} for ex in examples],\n",
    "        outputs=[{\"response\": ex[\"response\"]} for ex in examples],\n",
    "        dataset_id=dataset.id # Associate examples with the created dataset.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Definir la lógica de la aplicación a evaluar\n",
    "\n",
    "Necesitamos una función que encapsule la ejecución de nuestro LangGraph y devuelva la respuesta final en un formato adecuado para la evaluación. Dado que nuestro grafo incluye una interrupción `human_input` para la verificación del cliente, debemos manejar esto en nuestra función de evaluación. Simularemos que el usuario proporciona su ID de cliente (por ejemplo, \"Mi ID de cliente es 10\") para permitir que el grafo continúe más allá del paso de verificación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid # For generating unique thread IDs\n",
    "from langgraph.types import Command # For resuming graph execution after an interrupt\n",
    "\n",
    "graph = multi_agent_final_graph # Reference our complete, final multi-agent graph\n",
    "\n",
    "async def run_graph(inputs: dict):\n",
    "    \"\"\"Run graph and track the final response for evaluation.\"\"\"\n",
    "    # Creating a unique thread ID for each evaluation run to ensure isolation.\n",
    "    thread_id = uuid.uuid4()\n",
    "    # Configuration for the graph invocation. User ID '10' is used here for a specific test scenario.\n",
    "    configuration = {\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"10\"}}\n",
    "\n",
    "    # Invoke the graph with the initial user question.\n",
    "    # This invocation will likely hit the `human_input` node and interrupt if `customer_id` is not present.\n",
    "    result = await graph.ainvoke({\"messages\": [\n",
    "        { \"role\": \"user\", \"content\": inputs['question']}]}, config = configuration)\n",
    "    \n",
    "    # After the first invocation, if an interrupt occurred, resume it.\n",
    "    # We explicitly provide a (simulated) customer ID to pass the verification step.\n",
    "    # The `thread_id` in the config must match the initial invocation to resume the correct state.\n",
    "    result = await graph.ainvoke(Command(resume=\"My customer ID is 10\"), config={\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"10\"}})\n",
    "    \n",
    "    # Return the content of the last message in the conversation as the final response.\n",
    "    # This is the output that will be evaluated against the dataset's `response`.\n",
    "    return {\"response\": result['messages'][-1].content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the Evaluator for Final Response\n",
    "\n",
    "Evaluators are functions that take the application's output, the original input, and sometimes the reference output, and return a score or feedback. We can use pre-built evaluators or define our own.\n",
    "\n",
    "##### Using a Pre-built Evaluator (OpenEvals)\n",
    "LangSmith integrates with `openevals`, a library providing ready-to-use LLM-as-a-judge evaluators. The `create_llm_as_judge` function sets up an evaluator that uses an LLM to score responses based on a given prompt (e.g., `CORRECTNESS_PROMPT`).\n",
    "\n",
    "##### Defining a Custom Evaluator (LLM-as-a-Judge)\n",
    "For more specific or nuanced evaluation criteria, you can define your own LLM-as-a-judge evaluator. This involves:\n",
    "1.  **Custom Instructions**: A detailed prompt for the LLM that explains its role as a grader and the criteria for scoring.\n",
    "2.  **Structured Output Schema**: A Pydantic `BaseModel` or `TypedDict` to enforce the format of the LLM's grading output (e.g., `is_correct: bool`, `reasoning: str`).\n",
    "3.  **Evaluator Function**: A Python function that calls the structured LLM with the prompt, inputs, and reference outputs, then extracts the relevant score.\n",
    "\n",
    "This approach gives you maximum flexibility over how your agent's responses are judged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openevals.llm import create_llm_as_judge # Import the utility to create LLM-as-judge evaluators\n",
    "from openevals.prompts import CORRECTNESS_PROMPT # Import a pre-defined prompt for correctness evaluation\n",
    "\n",
    "# Create an LLM-as-judge evaluator for correctness using the pre-built `CORRECTNESS_PROMPT`.\n",
    "# `feedback_key=\"correctness\"` sets the name of the score reported in LangSmith.\n",
    "# `judge=model` specifies which LLM to use for judging.\n",
    "correctness_evaluator = create_llm_as_judge(\n",
    "    prompt=CORRECTNESS_PROMPT,\n",
    "    feedback_key=\"correctness\",\n",
    "    judge=llm,\n",
    ")\n",
    "\n",
    "# Print the content of the pre-defined correctness prompt to understand its instructions.\n",
    "print(CORRECTNESS_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom definition of LLM-as-judge instructions.\n",
    "    # This prompt provides specific guidelines for the LLM acting as a grader, focusing on factual accuracy.\n",
    "grader_instructions = \"\"\"You are a teacher grading a quiz.\n",
    "\n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.\n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.\n",
    "(2) Ensure that the student response does not contain any conflicting statements.\n",
    "(3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the ground truth response.\n",
    "\n",
    "Correctness:\n",
    "True means that the student's response meets all of the criteria.\n",
    "False means that the student's response does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.\"\"\"\n",
    "\n",
    "# Define the schema for the LLM-as-judge's output using TypedDict.\n",
    "# This ensures the grading output is structured with a reasoning and a boolean correctness score.\n",
    "class Grade(TypedDict):\n",
    "    \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\"\n",
    "    reasoning: Annotated[str, ..., \"Explain your reasoning for whether the actual response is correct or not.\"]\n",
    "    is_correct: Annotated[bool, ..., \"True if the student response is mostly or exactly correct, otherwise False.\"]\n",
    "\n",
    "# Configure the judge LLM to output structured data according to the `Grade` schema.\n",
    "# `method=\"json_schema\"` ensures JSON-based structured output, `strict=True` enforces strict adherence.\n",
    "grader_llm = llm.with_structured_output(Grade, method=\"json_schema\", strict=True)\n",
    "\n",
    "# Define the custom evaluator function `final_answer_correct`.\n",
    "# This function takes inputs, outputs (from our `run_graph`), and reference outputs (from the dataset).\n",
    "async def final_answer_correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\n",
    "    # Construct the user prompt for the grader LLM, combining the question, ground truth, and student response.\n",
    "    user = f\"\"\"QUESTION: {inputs['question']}\n",
    "    GROUND TRUTH RESPONSE: {reference_outputs['response']}\n",
    "    STUDENT RESPONSE: {outputs['response']}\"\"\"\n",
    "\n",
    "    # Invoke the structured grader LLM with the system instructions and the user prompt.\n",
    "    # Awaiting the async call as LLM invocations are typically async.\n",
    "    grade = await grader_llm.ainvoke([{\"role\": \"system\", \"content\": grader_instructions}, {\"role\": \"user\", \"content\": user}])\n",
    "    \n",
    "    # Return the `is_correct` boolean from the grader's output as the evaluation score.\n",
    "    return grade[\"is_correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run the Final Response Evaluation\n",
    "\n",
    "Now we're ready to run our evaluation job using the LangSmith client. The `aevaluate` method orchestrates the entire process:\n",
    "\n",
    "1.  It fetches inputs from the specified `data` (our dataset).\n",
    "2.  For each input, it calls our `run_graph` function.\n",
    "3.  It then passes the `run_graph`'s output, along with the original input and dataset's reference output, to each defined `evaluator`.\n",
    "4.  All results are logged and visible in your LangSmith project, providing a comprehensive report of your agent's performance.\n",
    "\n",
    "Key parameters:\n",
    "*   `run_graph`: Our target function to be evaluated.\n",
    "*   `data`: The name of the dataset created in LangSmith.\n",
    "*   `evaluators`: A list of evaluator functions to apply.\n",
    "*   `experiment_prefix`: A prefix for the experiment name in LangSmith, useful for organizing runs.\n",
    "*   `num_repetitions`: How many times to run each example. (For more robust results, typically >1)\n",
    "*   `max_concurrency`: The maximum number of parallel runs (useful for speeding up evaluation).\n",
    "\n",
    "Upon completion, you can navigate to your LangSmith project to view detailed traces and aggregated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation job asynchronously using the LangSmith client.\n",
    "    # This will execute `run_graph` for each example in the dataset and apply the specified evaluators.\n",
    "experiment_results = await client.aevaluate(\n",
    "    run_graph,                        # The asynchronous function that runs our graph and returns its output\n",
    "    data=dataset_name,                # The name of the LangSmith dataset to use for inputs and references\n",
    "    evaluators=[final_answer_correct, correctness_evaluator], # List of evaluator functions to apply\n",
    "    experiment_prefix=\"agent-Llama-e2e\", # A prefix for the experiment name in LangSmith for better organization\n",
    "    num_repetitions=1,                # Number of times to run each example (1 for quick testing)\n",
    "    max_concurrency=5,                # Maximum concurrent runs to optimize evaluation speed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Single Step of the Agent\n",
    "\n",
    "While end-to-end evaluation is important, it can be challenging to debug. Sometimes, an agent might fail overall, but you don't know *which* specific decision or action led to the failure. **Single-step evaluation** allows you to test individual components or critical decisions within your agent's workflow in isolation, similar to unit testing in software development.\n",
    "\n",
    "For our multi-agent system, a critical single step is the **supervisor's routing decision**: does it correctly send the query to the music agent or the invoice agent?\n",
    "\n",
    "*   **Input**: The specific input to that single step (e.g., the user message that the supervisor receives).\n",
    "*   **Output**: The direct output of that step (e.g., the name of the agent the supervisor chose to route to).\n",
    "\n",
    "![Single Step Evaluation](images/single-step.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a Dataset for Single-Step Evaluation\n",
    "\n",
    "For single-step evaluation, our dataset's `inputs` will be the user message, and the `outputs` will be the *expected routing decision* (i.e., the name of the sub-agent that should be activated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define examples for single-step evaluation, focusing on the supervisor's routing.\n",
    "    # `messages`: The input to the supervisor (the user's query).\n",
    "    # `route`: The expected output of the supervisor (the name of the sub-agent it should route to).\n",
    "examples = [\n",
    "    {\n",
    "        \"messages\": \"My customer ID is 1. What's my most recent purchase? and What albums does the catalog have by U2?\", \n",
    "        \"route\": 'transfer_to_invoice_information_subagent' # Expects initial routing to invoice agent\n",
    "    },\n",
    "    {\n",
    "        \"messages\": \"What songs do you have by U2?\", \n",
    "        \"route\": 'transfer_to_music_catalog_subagent' # Expects routing to music agent\n",
    "    },\n",
    "    {\n",
    "        \"messages\": \"My name is Aaron Mitchell. My number associated with my account is +1 (204) 452-6452. I am trying to find the invoice number for my most recent song purchase. Could you help me with it?\", \n",
    "        \"route\": 'transfer_to_invoice_information_subagent' # Expects routing to invoice agent\n",
    "    },\n",
    "    {\n",
    "        \"messages\": \"Who recorded Wish You Were Here again? What other albums by them do you have?\", \n",
    "        \"route\": 'transfer_to_music_catalog_subagent' # Expects routing to music agent\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "dataset_name = \"LangGraph 101 Multi-Agent: Single-Step\" # Name for this specific dataset.\n",
    "# Check and create the dataset in LangSmith if it doesn't already exist.\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs = [{\"messages\": ex[\"messages\"]} for ex in examples],\n",
    "        outputs = [{\"route\": ex[\"route\"]} for ex in examples],\n",
    "        dataset_id=dataset.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Application Logic to Evaluate (Single Step)\n",
    "\n",
    "To evaluate only the supervisor's routing, we need to run our `supervisor_prebuilt` graph but *interrupt* its execution immediately after the supervisor makes its routing decision, before any sub-agents are actually invoked. LangGraph's `interrupt_before` argument is perfect for this.\n",
    "\n",
    "The `interrupt_before` parameter tells the graph to pause execution right before entering the specified nodes. In this case, we want to pause before `music_catalog_subagent` or `invoice_information_subagent` are invoked. This allows us to inspect the state and determine what the supervisor decided to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_supervisor_routing(inputs: dict):\n",
    "    \"\"\"Runs the supervisor graph up to the point of routing and returns the chosen route.\"\"\"\n",
    "    # Invoke the `supervisor_prebuilt` graph.\n",
    "    # `interrupt_before` specifies that the graph should pause execution just before entering \n",
    "    # either the music or invoice sub-agent nodes. This captures the routing decision.\n",
    "    # A dummy `user_id` and `thread_id` are provided for configuration, as the supervisor itself doesn't need real verification here.\n",
    "    result = await supervisor_prebuilt.ainvoke(\n",
    "        {\"messages\": [HumanMessage(content=inputs['messages'])]},\n",
    "        interrupt_before=[\"music_catalog_subagent\", \"invoice_information_subagent\"],\n",
    "        config={\"configurable\": {\"thread_id\": uuid.uuid4(), \"user_id\" : \"10\"}}\n",
    "    )\n",
    "    \n",
    "    # The name of the last message (which is typically the `tool_call` or `message` that represents the routing decision)\n",
    "    # should correspond to the name of the next chosen sub-agent. This is how the supervisor indicates its routing.\n",
    "    return {\"route\": result[\"messages\"][-1].name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the Evaluator for Single Step\n",
    "\n",
    "For this single-step evaluation, a simple exact match evaluator is sufficient. It will check if the `route` output by our `run_supervisor_routing` function exactly matches the `route` defined in our dataset's `reference_outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Evaluator function to check if the agent chose the correct route.\"\"\"\n",
    "    # Compares the 'route' returned by our `run_supervisor_routing` function\n",
    "    # with the 'route' specified in the ground truth dataset.\n",
    "    return outputs['route'] == reference_outputs[\"route\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run the Single Step Evaluation\n",
    "\n",
    "Now we execute the single-step evaluation using `client.aevaluate`, similar to the final response evaluation, but with our specialized function and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = await client.aevaluate(\n",
    "    run_supervisor_routing,           # Our function that runs only the supervisor routing step\n",
    "    data=dataset_name,                # The dataset specifically for single-step routing evaluation\n",
    "    evaluators=[correct],\n",
    "    experiment_prefix=\"agent-Llama-singlestep\",\n",
    "    max_concurrency=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Trajectory of the Agent\n",
    "\n",
    "**Trajectory evaluation** takes a deeper look into the agent's internal workings. Instead of just assessing the final output or a single step, it evaluates the entire sequence of steps (the \"trajectory\") an agent takes to arrive at its answer. This is particularly useful for complex agents where the *process* of reaching a solution is as important as the solution itself (e.g., ensuring a specific set of tools are used in a particular order).\n",
    "\n",
    "*   **Input**: The initial user query to the overall agent.\n",
    "*   **Output**: A detailed list of all nodes/steps visited during the agent's execution.\n",
    "\n",
    "![Trajectory Evaluation](images/trajectory.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a Dataset for Trajectory Evaluation\n",
    "\n",
    "For trajectory evaluation, our dataset will contain the user `question` as input and an ordered list of `trajectory` (the expected sequence of node names) as the ground truth output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define examples for trajectory evaluation.\n",
    "    # `question`: The user's input.\n",
    "    # `trajectory`: The expected ordered list of node names visited by the graph.\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"My customer ID is 1. What's my most recent purchase? and What albums does the catalog have by U2?\",\n",
    "        \"trajectory\": [\"verify_info\", \"load_memory\", \"supervisor\", \"create_memory\"], # Expected path when customer ID is provided\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What songs do you have by U2?\",\n",
    "        \"trajectory\": [\"verify_info\", \"human_input\", \"verify_info\", \"load_memory\", \"supervisor\", \"create_memory\"], # Expected path with initial verification and resume\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"My name is Aaron Mitchell. My number associated with my account is +1 (204) 452-6452. I am trying to find the invoice number for my most recent song purchase. Could you help me with it?\",\n",
    "        \"trajectory\": [\"verify_info\", \"load_memory\", \"supervisor\", \"create_memory\"], # Expected path when customer ID is provided implicitly\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who recorded Wish You Were Here again? What other albums by them do you have?\",\n",
    "        \"trajectory\": [\"verify_info\", \"human_input\", \"verify_info\", \"load_memory\", \"supervisor\", \"create_memory\"], # Another example with initial verification and resume\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset_name = \"LangGraph 101 Multi-Agent: Trajectory Eval\" # Name for this dataset.\n",
    "\n",
    "# Check and create the dataset in LangSmith if it doesn't already exist.\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    client.create_examples(\n",
    "        inputs=[{\"question\": ex[\"question\"]} for ex in examples],\n",
    "        outputs=[{\"trajectory\": ex[\"trajectory\"]} for ex in examples],\n",
    "        dataset_id=dataset.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Application Logic to Evaluate (Trajectory)\n",
    "\n",
    "To capture the full trajectory, we will use `graph.astream(stream_mode=\"debug\")`. The `debug` stream mode yields detailed `chunk` objects for each step in the graph, including the `task` chunks which contain the name of the node being executed. We'll collect these node names into a list to form the actual trajectory.\n",
    "\n",
    "Similar to the final response evaluation, we need to handle the `human_input` interrupt by resuming the graph with dummy input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = multi_agent_final_graph # Reference our complete multi-agent graph\n",
    "\n",
    "async def run_graph(inputs: dict) -> dict:\n",
    "    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\n",
    "    trajectory = [] # List to store the names of nodes visited\n",
    "    thread_id = uuid.uuid4() # Unique ID for the current thread\n",
    "    # Configuration for the graph invocation, including a dummy user_id for verification step.\n",
    "    configuration = {\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"10\"}}\n",
    "\n",
    "    # First, run the graph for the initial question. `astream` allows us to iterate through chunks.\n",
    "    # `stream_mode=\"debug\"` provides detailed information about each step, including node names.\n",
    "    async for chunk in graph.astream({\"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": inputs['question'],\n",
    "            }\n",
    "        ]}, config = configuration, stream_mode=\"debug\"):\n",
    "        # Check if the chunk type is 'task' (indicating a node execution).\n",
    "        if chunk['type'] == 'task':\n",
    "            # Append the name of the executed node to our trajectory list.\n",
    "            trajectory.append(chunk['payload']['name'])\n",
    "\n",
    "    # If the graph paused for human input, resume it with a dummy customer ID.\n",
    "    async for chunk in graph.astream(Command(resume=\"My customer ID is 10\"), config = configuration, stream_mode=\"debug\"):\n",
    "        if chunk['type'] == 'task':\n",
    "            trajectory.append(chunk['payload']['name'])\n",
    "            \n",
    "    # Return the collected trajectory list.\n",
    "    return {\"trajectory\": trajectory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the Evaluator for Trajectory\n",
    "\n",
    "For trajectory evaluation, we'll define two custom evaluators:\n",
    "\n",
    "1.  **`evaluate_exact_match`**: This simple evaluator checks if the `actual trajectory` exactly matches the `expected trajectory` from the dataset. It provides a binary score (True/False).\n",
    "2.  **`evaluate_extra_steps`**: This more sophisticated evaluator counts the number of \"unmatched\" or \"extra\" steps taken by the agent that were not present in the reference trajectory. This can indicate inefficiency or unexpected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_exact_match(outputs: dict, reference_outputs: dict):\n",
    "    \"\"\"Evaluate whether the trajectory exactly matches the expected output\"\"\"\n",
    "    return {\n",
    "        \"key\": \"exact_match\", # The key for this evaluation metric in LangSmith\n",
    "        \"score\": outputs[\"trajectory\"] == reference_outputs[\"trajectory\"] # True if trajectories are identical\n",
    "    }\n",
    "\n",
    "def evaluate_extra_steps(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Evaluate the number of unmatched steps in the agent's output trajectory compared to the reference.\"\"\"\n",
    "    i = j = 0 # Pointers for reference trajectory (i) and actual output trajectory (j)\n",
    "    unmatched_steps = 0 # Counter for steps in output not found in reference sequence\n",
    "\n",
    "    # Iterate through both trajectories to find matches and count mismatches.\n",
    "    while i < len(reference_outputs['trajectory']) and j < len(outputs['trajectory']):\n",
    "        if reference_outputs['trajectory'][i] == outputs['trajectory'][j]:\n",
    "            i += 1  # Match found, move to the next step in reference trajectory\n",
    "        else:\n",
    "            unmatched_steps += 1  # Step in output is not the expected one, count as unmatched\n",
    "        j += 1  # Always move to the next step in outputs trajectory\n",
    "\n",
    "    # After the loop, if there are remaining steps in the output trajectory,\n",
    "    # they are all considered unmatched (extra steps taken by the agent).\n",
    "    unmatched_steps += len(outputs['trajectory']) - j\n",
    "\n",
    "    return {\n",
    "        \"key\": \"unmatched_steps\", # The key for this evaluation metric\n",
    "        \"score\": unmatched_steps, # The count of unmatched steps\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run the Trajectory Evaluation\n",
    "\n",
    "Finally, we run the trajectory evaluation using our specialized `run_graph` function and the two custom trajectory evaluators. The results will be uploaded to LangSmith, where you can analyze the sequence of node executions and compare them against your expected paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = await client.aevaluate(\n",
    "    run_graph,                        # Our function that collects the full trajectory\n",
    "    data=dataset_name,                # The dataset specifically for trajectory evaluation\n",
    "    evaluators=[evaluate_extra_steps, evaluate_exact_match], # Our custom trajectory evaluators\n",
    "    experiment_prefix=\"agent-Llama-trajectory\", # Prefix for the experiment name in LangSmith\n",
    "    num_repetitions=1,\n",
    "    max_concurrency=4,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
